{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNNshio.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiomasa1218/Colaboratory/blob/master/CNNshio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "u2DXHgAq-QdK",
        "colab_type": "code",
        "outputId": "3f218836-9759-4b60-c65e-ce8b6192d566",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "trfp-xvj-TgB",
        "colab_type": "code",
        "outputId": "6e971e9a-e573-4a0d-829c-44a953acd7be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# drive mean root directory of  google drive\n",
        "!ls ./gdrive/'My Drive'/'Kumamoto-Univ'/'Graduationwork'/'exefolder'/'test_folder_name'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "carpet1_2.0_afterlinear   sponge-y_2.0_afterlinear\n",
            "carpet2_2.0_afterlinear   stonetile1_2.0_afterlinear\n",
            "carpet3_2.0_afterlinear   whiteitile1_2.0_afterlinear\n",
            "sponge-g_2.0_afterlinear  woodtile1_2.0_afterlinear\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Zw_5NFXE-WTR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7643f643-9698-431f-d52d-ef0d51ae1e25"
      },
      "cell_type": "code",
      "source": [
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# check auth\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K    1% |▎                               | 10kB 23.0MB/s eta 0:00:01\r\u001b[K    2% |▋                               | 20kB 4.7MB/s eta 0:00:01\r\u001b[K    3% |█                               | 30kB 6.7MB/s eta 0:00:01\r\u001b[K    4% |█▎                              | 40kB 4.2MB/s eta 0:00:01\r\u001b[K    5% |█▋                              | 51kB 5.1MB/s eta 0:00:01\r\u001b[K    6% |██                              | 61kB 6.1MB/s eta 0:00:01\r\u001b[K    7% |██▎                             | 71kB 7.0MB/s eta 0:00:01\r\u001b[K    8% |██▋                             | 81kB 7.8MB/s eta 0:00:01\r\u001b[K    9% |███                             | 92kB 8.6MB/s eta 0:00:01\r\u001b[K    10% |███▎                            | 102kB 6.8MB/s eta 0:00:01\r\u001b[K    11% |███▋                            | 112kB 6.8MB/s eta 0:00:01\r\u001b[K    12% |████                            | 122kB 9.0MB/s eta 0:00:01\r\u001b[K    13% |████▎                           | 133kB 8.9MB/s eta 0:00:01\r\u001b[K    14% |████▋                           | 143kB 15.8MB/s eta 0:00:01\r\u001b[K    15% |█████                           | 153kB 15.9MB/s eta 0:00:01\r\u001b[K    16% |█████▎                          | 163kB 15.7MB/s eta 0:00:01\r\u001b[K    17% |█████▋                          | 174kB 15.2MB/s eta 0:00:01\r\u001b[K    18% |██████                          | 184kB 15.3MB/s eta 0:00:01\r\u001b[K    19% |██████▎                         | 194kB 15.4MB/s eta 0:00:01\r\u001b[K    20% |██████▋                         | 204kB 39.9MB/s eta 0:00:01\r\u001b[K    21% |███████                         | 215kB 20.2MB/s eta 0:00:01\r\u001b[K    22% |███████▎                        | 225kB 20.4MB/s eta 0:00:01\r\u001b[K    23% |███████▋                        | 235kB 21.1MB/s eta 0:00:01\r\u001b[K    24% |████████                        | 245kB 20.7MB/s eta 0:00:01\r\u001b[K    25% |████████▎                       | 256kB 20.7MB/s eta 0:00:01\r\u001b[K    26% |████████▋                       | 266kB 19.3MB/s eta 0:00:01\r\u001b[K    27% |█████████                       | 276kB 20.3MB/s eta 0:00:01\r\u001b[K    29% |█████████▎                      | 286kB 20.2MB/s eta 0:00:01\r\u001b[K    30% |█████████▋                      | 296kB 20.1MB/s eta 0:00:01\r\u001b[K    31% |██████████                      | 307kB 20.8MB/s eta 0:00:01\r\u001b[K    32% |██████████▎                     | 317kB 44.5MB/s eta 0:00:01\r\u001b[K    33% |██████████▋                     | 327kB 46.0MB/s eta 0:00:01\r\u001b[K    34% |███████████                     | 337kB 47.1MB/s eta 0:00:01\r\u001b[K    35% |███████████▎                    | 348kB 44.7MB/s eta 0:00:01\r\u001b[K    36% |███████████▋                    | 358kB 44.3MB/s eta 0:00:01\r\u001b[K    37% |████████████                    | 368kB 54.8MB/s eta 0:00:01\r\u001b[K    38% |████████████▎                   | 378kB 54.6MB/s eta 0:00:01\r\u001b[K    39% |████████████▋                   | 389kB 55.9MB/s eta 0:00:01\r\u001b[K    40% |█████████████                   | 399kB 29.0MB/s eta 0:00:01\r\u001b[K    41% |█████████████▎                  | 409kB 28.7MB/s eta 0:00:01\r\u001b[K    42% |█████████████▋                  | 419kB 28.5MB/s eta 0:00:01\r\u001b[K    43% |██████████████                  | 430kB 27.8MB/s eta 0:00:01\r\u001b[K    44% |██████████████▎                 | 440kB 27.4MB/s eta 0:00:01\r\u001b[K    45% |██████████████▋                 | 450kB 27.9MB/s eta 0:00:01\r\u001b[K    46% |███████████████                 | 460kB 27.9MB/s eta 0:00:01\r\u001b[K    47% |███████████████▎                | 471kB 28.2MB/s eta 0:00:01\r\u001b[K    48% |███████████████▋                | 481kB 27.9MB/s eta 0:00:01\r\u001b[K    49% |████████████████                | 491kB 27.4MB/s eta 0:00:01\r\u001b[K    50% |████████████████▎               | 501kB 50.7MB/s eta 0:00:01\r\u001b[K    51% |████████████████▋               | 512kB 47.7MB/s eta 0:00:01\r\u001b[K    52% |█████████████████               | 522kB 47.6MB/s eta 0:00:01\r\u001b[K    53% |█████████████████▎              | 532kB 49.6MB/s eta 0:00:01\r\u001b[K    54% |█████████████████▋              | 542kB 50.7MB/s eta 0:00:01\r\u001b[K    55% |██████████████████              | 552kB 54.3MB/s eta 0:00:01\r\u001b[K    57% |██████████████████▎             | 563kB 54.9MB/s eta 0:00:01\r\u001b[K    58% |██████████████████▋             | 573kB 54.4MB/s eta 0:00:01\r\u001b[K    59% |███████████████████             | 583kB 55.7MB/s eta 0:00:01\r\u001b[K    60% |███████████████████▎            | 593kB 56.5MB/s eta 0:00:01\r\u001b[K    61% |███████████████████▋            | 604kB 56.8MB/s eta 0:00:01\r\u001b[K    62% |████████████████████            | 614kB 62.0MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▎           | 624kB 63.6MB/s eta 0:00:01\r\u001b[K    64% |████████████████████▋           | 634kB 63.8MB/s eta 0:00:01\r\u001b[K    65% |█████████████████████           | 645kB 62.0MB/s eta 0:00:01\r\u001b[K    66% |█████████████████████▎          | 655kB 60.7MB/s eta 0:00:01\r\u001b[K    67% |█████████████████████▋          | 665kB 44.4MB/s eta 0:00:01\r\u001b[K    68% |██████████████████████          | 675kB 43.3MB/s eta 0:00:01\r\u001b[K    69% |██████████████████████▎         | 686kB 43.6MB/s eta 0:00:01\r\u001b[K    70% |██████████████████████▋         | 696kB 44.3MB/s eta 0:00:01\r\u001b[K    71% |███████████████████████         | 706kB 44.0MB/s eta 0:00:01\r\u001b[K    72% |███████████████████████▎        | 716kB 44.7MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▋        | 727kB 44.8MB/s eta 0:00:01\r\u001b[K    74% |████████████████████████        | 737kB 44.2MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████▎       | 747kB 45.0MB/s eta 0:00:01\r\u001b[K    76% |████████████████████████▋       | 757kB 45.7MB/s eta 0:00:01\r\u001b[K    77% |████████████████████████▉       | 768kB 63.5MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████▏      | 778kB 66.1MB/s eta 0:00:01\r\u001b[K    79% |█████████████████████████▌      | 788kB 65.2MB/s eta 0:00:01\r\u001b[K    80% |█████████████████████████▉      | 798kB 64.5MB/s eta 0:00:01\r\u001b[K    81% |██████████████████████████▏     | 808kB 64.1MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▌     | 819kB 62.8MB/s eta 0:00:01\r\u001b[K    83% |██████████████████████████▉     | 829kB 63.4MB/s eta 0:00:01\r\u001b[K    85% |███████████████████████████▏    | 839kB 65.1MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▌    | 849kB 66.0MB/s eta 0:00:01\r\u001b[K    87% |███████████████████████████▉    | 860kB 58.8MB/s eta 0:00:01\r\u001b[K    88% |████████████████████████████▏   | 870kB 57.8MB/s eta 0:00:01\r\u001b[K    89% |████████████████████████████▌   | 880kB 57.9MB/s eta 0:00:01\r\u001b[K    90% |████████████████████████████▉   | 890kB 58.4MB/s eta 0:00:01\r\u001b[K    91% |█████████████████████████████▏  | 901kB 59.1MB/s eta 0:00:01\r\u001b[K    92% |█████████████████████████████▌  | 911kB 60.0MB/s eta 0:00:01\r\u001b[K    93% |█████████████████████████████▉  | 921kB 60.2MB/s eta 0:00:01\r\u001b[K    94% |██████████████████████████████▏ | 931kB 59.9MB/s eta 0:00:01\r\u001b[K    95% |██████████████████████████████▌ | 942kB 59.7MB/s eta 0:00:01\r\u001b[K    96% |██████████████████████████████▉ | 952kB 59.4MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████▏| 962kB 68.0MB/s eta 0:00:01\r\u001b[K    98% |███████████████████████████████▌| 972kB 69.1MB/s eta 0:00:01\r\u001b[K    99% |███████████████████████████████▉| 983kB 68.5MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 993kB 22.1MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "u2Wf2ubj-atT",
        "colab_type": "code",
        "outputId": "d310f678-b27c-40e3-c501-6f56431bf224",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "cd gdrive/'My Drive'/'Kumamoto-Univ'/'Graduationwork'/'exefolder'/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Kumamoto-Univ/Graduationwork/exefolder\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "H9c6rYVo-JKl",
        "colab_type": "code",
        "outputId": "5a275a8e-2963-4c59-884d-3320ae633ebe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6514
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import csv\n",
        "import pandas as pd\n",
        "import input_data\n",
        "import time\n",
        "import datetime\n",
        "from sklearn.metrics import confusion_matrix,recall_score,precision_score,f1_score\n",
        "\n",
        "#フィルタ作成\n",
        "def weight_variable(shape):\n",
        "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "    return tf.Variable(initial)\n",
        "     \n",
        "def weight_variable_he(shape,nodes):\n",
        "    initial = tf.truncated_normal(shape, stddev=np.sqrt(2.0/nodes))\n",
        "    return tf.Variable(initial)\n",
        "     \n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.1, shape=shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def conv2d(x,W):\n",
        "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='VALID')\n",
        "\n",
        "def max_pool_2x2(x):\n",
        "    return tf.nn.max_pool(x, ksize = [1,1,2,1], strides=[1,1,2,1],padding = 'VALID')\n",
        "\n",
        "def batch_normalization(shape, input):\n",
        "    eps = 1e-5\n",
        "    gamma = weight_variable([shape])\n",
        "    beta = weight_variable([shape])\n",
        "    mean,variance = tf.nn.moments(input,[0])\n",
        "    return gamma * (input - mean) / tf.sqrt(variance+eps) + beta\n",
        "\n",
        "def Activation(x):\n",
        "    datas = tf.nn.relu(x)\n",
        "    #datas = tf.nn.tanh(x)\n",
        "    return datas\n",
        "\n",
        "\n",
        "\n",
        "#define\n",
        "#Data_Classes = 8#学習クラス数\n",
        "steps = 3000 #エポック数　学習の回数\n",
        "BATCH_SIZE = 600 #ミニバッチの一回の学習で使う量\n",
        "#TBATCH_SIZE = 10\n",
        "Validation = 0.8  #  ここの値で分割 ex. Validation = 0.4 →　0.4:0.6　に分割\n",
        "CaptureNumber = 200 #データ数　ミニバッチの大きさ、初期は２００個のデータ\n",
        "drop = 1.0 #dropoutの係数、NNのノードがこれをかけた数になる\n",
        "\n",
        "allCSize = 4608 #全結合サイズ\n",
        "filterSize = 64 #畳みこみフィルタ（カーネル）数 今回は 1 x filiterSize の大きさ\n",
        "\n",
        "# 訓練データのフォルダのパス\n",
        "train_folder_name = \"train_folder_name\"\n",
        "\n",
        "# 検証データのフォルダのパス\n",
        "test_folder_name = \"test_folder_name\"\n",
        "\n",
        "#全訓練データとそのラベル\n",
        "All_Datas = []\n",
        "All_Label = []\n",
        "\n",
        "#外部検証データとそのラベル\n",
        "Ex_TestDatas=[]\n",
        "Ex_Label = []\n",
        "\n",
        "#並び順をシャッフルしたあとの訓練データ\n",
        "All_SDatas =[]\n",
        "All_SLabel = []\n",
        "\n",
        "#分割後の訓練データ，学習するほう\n",
        "train_data = []\n",
        "train_label = []\n",
        "\n",
        "#分割後の訓練データ，モデル評価するほう\n",
        "test_data = []\n",
        "test_label = []\n",
        "\n",
        "#フォルダ中身のファイル名を取得\n",
        "trainFolder = os.listdir(train_folder_name)\n",
        "testFolder = os.listdir(test_folder_name)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#学習クラス数\n",
        "Data_Classes = len(trainFolder)\n",
        "\n",
        "# .DS_Storeがあるか検知\n",
        "for i,d in enumerate(trainFolder):\n",
        "    if d == \".DS_Store\":\n",
        "        Data_Classes = Data_Classes - 1\n",
        "        print(\"DS!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "\n",
        "\n",
        "print(Data_Classes)\n",
        "print(len(testFolder))\n",
        "print(train_folder_name)\n",
        "print(test_folder_name)\n",
        "\n",
        "#フォルダごとにみる\n",
        "#訓練データの読み込み\n",
        "\n",
        "dsflag = 0\n",
        "\n",
        "for i,d in enumerate(trainFolder):\n",
        "    #one_hot_vector生成用\n",
        "    tmp = np.zeros(Data_Classes)\n",
        "    if d != \".DS_Store\":\n",
        "        #フォルダ内のファイルのリストを取得\n",
        "        files = os.listdir(train_folder_name + '/'+d)\n",
        "        #print(files)\n",
        "\n",
        "        #one_hot_vectorを作成\n",
        "        tmp[i-dsflag] = 1\n",
        "        #ファイル毎にみる\n",
        "        for f in files:\n",
        "                            \n",
        "            #.DS_Storeをどかす\n",
        "            if f != \".DS_Store\":\n",
        "                #どのファイルを見ているか確認用\n",
        "                #print(\"load:\"+f)\n",
        "\n",
        "                datafile_path = train_folder_name + '/' + d+'/'+f\n",
        "\n",
        "                #csvから読み込み\n",
        "                data = np.loadtxt(datafile_path, delimiter=\",\")     #pbldata用\n",
        "\n",
        "                #3x200のデータに整形\n",
        "                x_csv = data[1, :]\n",
        "                y_csv = data[2, :]\n",
        "                z_csv = data[3, :]\n",
        "                \n",
        "#                 start = random.randint(0, len(x_csv) - CaptureNumber)\n",
        "#                 end = start + CaptureNumber\n",
        "\n",
        "#                 x_data = x_csv[start:end]\n",
        "#                 y_data = y_csv[start:end]\n",
        "#                 z_data = z_csv[start:end]\n",
        "\n",
        "\n",
        "#                 flatdata = []\n",
        "#                 flatdata.append(x_data)\n",
        "#                 flatdata.append(y_data)\n",
        "#                 flatdata.append(z_data)\n",
        "                \n",
        "                flatdata = []\n",
        "                flatdata.append(x_csv)\n",
        "                flatdata.append(y_csv)\n",
        "                flatdata.append(z_csv)\n",
        "          \n",
        "                All_Datas.append(flatdata)\n",
        "                #one_hot_vectorをラベルに追加\n",
        "                All_Label.append(tmp)\n",
        "            else:\n",
        "                print(\".ds_storeを除去\")\n",
        "    else:\n",
        "        print(\".ds_storeを抹殺\")\n",
        "        print(i)\n",
        "        dsflag = dsflag + 1\n",
        "\n",
        "\n",
        "dsflag = 0\n",
        "\n",
        "#検証データの読み込み\n",
        "for i, d in enumerate(testFolder):\n",
        "    #one_hot_vector生成用\n",
        "    tmp = np.zeros(Data_Classes)\n",
        "    if d != \".DS_Store\":\n",
        "        # フォルダ内のファイルのリストを取得\n",
        "        files = os.listdir(test_folder_name + '/' + d)\n",
        "        #print(files)\n",
        "\n",
        "        #one_hot_vectorを作成\n",
        "        tmp[i-dsflag] = 1\n",
        "        #ファイル毎にみる\n",
        "        for f in files:\n",
        "                            \n",
        "            #.DS_Storeをどかす\n",
        "            if f != \".DS_Store\":\n",
        "                #どのファイルを見ているか確認用\n",
        "                #print(\"load:\"+f)\n",
        "\n",
        "                datafile_path = test_folder_name + '/' + d+'/'+f\n",
        "\n",
        "                #csvから読み込み\n",
        "                data = np.loadtxt(datafile_path, delimiter=\",\")     #pbldata用\n",
        "\n",
        "                 #3x200のデータに整形\n",
        "                x_csv = data[1, :]\n",
        "                y_csv = data[2, :]\n",
        "                z_csv = data[3, :]\n",
        "                \n",
        "                start = random.randint(0, len(x_csv) - CaptureNumber)\n",
        "                end = start + CaptureNumber\n",
        "\n",
        "                x_data = x_csv[start:end]\n",
        "                y_data = y_csv[start:end]\n",
        "                z_data = z_csv[start:end]\n",
        "\n",
        "\n",
        "                flatdata = []\n",
        "                flatdata.append(x_data)\n",
        "                flatdata.append(y_data)\n",
        "                flatdata.append(z_data)\n",
        "\n",
        "                \n",
        "                Ex_TestDatas.append(flatdata)\n",
        "                #one_hot_vectorをラベルに追加\n",
        "                Ex_Label.append(tmp)\n",
        "            else:\n",
        "                print(\".ds_storeを除去\")\n",
        "    else:\n",
        "        print(\"ex.ds_storeを抹殺\")\n",
        "        dsflag = dsflag + 1\n",
        "  \n",
        "print(\"Ex_Label\")\n",
        "print(Ex_Label[0])\n",
        "\n",
        "#訓練データ順列のシャッフル\n",
        "np.random.seed(seed=32)\n",
        "Rindex = np.random.permutation(list(range(len(All_Datas))))\n",
        "print(Rindex)\n",
        "for k in Rindex:\n",
        "    All_SDatas.append(All_Datas[k])\n",
        "    All_SLabel.append(All_Label[k])\n",
        "\n",
        "    \n",
        "#訓練データを学習に使うやつとモデルの評価に使うやつの２種に分ける\n",
        "numberV = int(len(All_SDatas)*(Validation)) #訓練データ数\n",
        "testV = len(All_SDatas) - numberV  #テストデータ数\n",
        "train_data = All_SDatas[:numberV] #0.９まで\n",
        "test_data = All_SDatas[numberV+1:]\n",
        "train_label = All_SLabel[:numberV]\n",
        "test_label = All_SLabel[numberV+1:]\n",
        "\n",
        "print(len(train_data))\n",
        "print(len(test_data))\n",
        "\n",
        "print(train_data[0])\n",
        "\n",
        "#----------\n",
        "#numpy行列へ\n",
        "#----------\n",
        "\n",
        "train_data = np.array(train_data)\n",
        "train_label = np.array(train_label)\n",
        "\n",
        "test_data = np.array(test_data)\n",
        "test_label = np.array(test_label)\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "x = tf.placeholder('float', shape=[None,3,CaptureNumber]) #3x200のデータ\n",
        "y_ = tf.placeholder('float', shape=[None, Data_Classes]) #正解ラベル\n",
        "keep_prob = tf.placeholder('float')\n",
        "                                     \n",
        "# -----------------------------------------\n",
        "# 畳み込みニューラルネットワーク\n",
        "#\n",
        "# (convconv -> bn -> pool -> drop) x3，\n",
        "# (fc->drop) x2\n",
        "# -----------------------------------------\n",
        "\n",
        "#第1ブロック\n",
        "W_conv1_1 = weight_variable([1,5,3,filterSize]) #1*5のフィルタ　入力３ｃｈ　出力６４枚\n",
        "b_conv1_1 = bias_variable([filterSize]) #出力６４ch\n",
        "x_image = tf.reshape(x,[-1,1,CaptureNumber,3]) #-１はreshapeに適切な数N　１*２００のデータ　３ｃｈ\n",
        "h_conv1_1 = conv2d(x_image, W_conv1_1)#+b_conv1_1 \n",
        "tan1_1 = Activation(h_conv1_1)\n",
        "bn1_1 = batch_normalization(filterSize,tan1_1)\n",
        "\n",
        "W_conv1_2 = weight_variable([1,5,filterSize,filterSize])\n",
        "b_conv1_2 = bias_variable([filterSize])\n",
        "h_conv1_2 = conv2d(bn1_1, W_conv1_2)#+b_conv1_2\n",
        "tan1_2 = Activation(h_conv1_2)\n",
        "bn1_2 = batch_normalization(filterSize,tan1_2)\n",
        "\n",
        "h_pool1 = max_pool_2x2(bn1_2)\n",
        "h_pool1_drop = tf.nn.dropout(h_pool1, keep_prob)\n",
        "\n",
        "\n",
        "# #第2ブロック\n",
        "W_conv2_1 = weight_variable([1,5,filterSize,filterSize*2])\n",
        "b_conv2_1 = bias_variable([filterSize*2])\n",
        "h_conv2_1 = conv2d(h_pool1_drop, W_conv2_1)#+b_conv2_1\n",
        "tan2_1 = Activation(h_conv2_1)\n",
        "bn2_1 = batch_normalization(filterSize*2,tan2_1)\n",
        "\n",
        "W_conv2_2 = weight_variable([1,5,filterSize*2,filterSize*2])\n",
        "b_conv2_2 = bias_variable([filterSize*2])\n",
        "h_conv2_2 = conv2d(bn2_1, W_conv2_2)#+b_conv2_2\n",
        "tan2_2 = Activation(h_conv2_2)\n",
        "bn2_2 = batch_normalization(filterSize*2,tan2_2)\n",
        "\n",
        "h_pool2 = max_pool_2x2(bn2_2)\n",
        "h_pool2_drop = tf.nn.dropout(h_pool2, keep_prob)\n",
        "\n",
        "#\n",
        "\n",
        "# #第3ブロック\n",
        "W_conv3_1 = weight_variable([1,5,filterSize*2,filterSize*4])\n",
        "b_conv3_1 = bias_variable([filterSize*4])\n",
        "h_conv3_1 = conv2d(h_pool2_drop, W_conv3_1)#+b_conv3_1\n",
        "tan3_1 = Activation(h_conv3_1)\n",
        "bn3_1 = batch_normalization(filterSize*4,tan3_1)\n",
        "\n",
        "W_conv3_2 = weight_variable([1,5,filterSize*4,filterSize*4])\n",
        "b_conv3_2 = bias_variable([filterSize*4])\n",
        "h_conv3_2 = conv2d(bn3_1, W_conv3_2)#+b_conv3_2\n",
        "tan3_2 = Activation(h_conv3_2)\n",
        "bn3_2 = batch_normalization(filterSize*4,tan3_2)\n",
        "\n",
        "\n",
        "h_pool3 = max_pool_2x2(bn3_2)\n",
        "h_pool3_drop = tf.nn.dropout(h_pool3, keep_prob)\n",
        "h_pool3_flat = tf.reshape(h_pool3_drop, [-1,allCSize])\n",
        "\n",
        "#全結合層\n",
        "W_fc1 = weight_variable([allCSize, allCSize])\n",
        "b_fc1 = bias_variable([allCSize])\n",
        "h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1)\n",
        "bn4 = batch_normalization(allCSize,h_fc1)\n",
        "\n",
        "# #dropout\n",
        "h_fc1_drop = tf.nn.dropout(bn4, 1.0)\n",
        "\n",
        "# #全結合層\n",
        "W_fc2 = weight_variable([allCSize, allCSize])\n",
        "b_fc2 = bias_variable([allCSize])\n",
        "\n",
        "h_fc2= tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
        "bn5 = batch_normalization(allCSize,h_fc1)\n",
        "\n",
        "# #dropout\n",
        "h_fc2_drop = tf.nn.dropout(bn5, 0.5)\n",
        "\n",
        "#softmaxで出力#\n",
        "W_fc4 = weight_variable([allCSize, Data_Classes])\n",
        "b_fc4 = bias_variable([Data_Classes])\n",
        "y_conv=tf.nn.softmax(tf.matmul(h_fc2_drop, W_fc4) + b_fc4)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#------------------\n",
        "# 評価と損失関数\n",
        "#------------------\n",
        "\n",
        "#L2ノルムを足そうとした残骸\n",
        "# L2_sqr = tf.nn.l2_loss(W_conv1) + tf.nn.l2_loss(W_conv2)+tf.nn.l2_loss(W_conv3) + tf.nn.l2_loss(W_fc1) + tf.nn.l2_loss(W_fc2)\n",
        "# lambda_2 = 0.01\n",
        "\n",
        "#クロスエントロピー誤差関数，1e-7を足して，勾配消失を防止\n",
        "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_*tf.log(y_conv+1e-7),reduction_indices=[1]))\n",
        "#loss = cross_entropy + lambda_2*L2_sqr\n",
        "# cross_entropy = -tf.reduce_sum(y_*tf.log(tf.clip_by_value(y_conv,1e-10,1.0)))\n",
        "\n",
        "#重みの最適化\n",
        "train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\n",
        "#　推定結果のy_convと正解ラベルy_が同じかどうか判定，correct_predictionがbool配列になってる\n",
        "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_,1))\n",
        "# 正答率\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "# run\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "\n",
        "\n",
        "#\n",
        "#学習器の保存処理\n",
        "saver = tf.train.Saver()\n",
        "ckpt = tf.train.get_checkpoint_state('./')\n",
        "cwd = os.getcwd()\n",
        "\n",
        "\n",
        "#処理時間計測\n",
        "startTime = datetime.datetime.today()\n",
        "\n",
        "#ログ\n",
        "learn = open('log.csv','w')\n",
        "result = open('result.txt','w')\n",
        "\n",
        "\n",
        "learn.write(\"class,step\\n\" )\n",
        "learn.write(\"%g,%g\\n\"%(Data_Classes,steps))\n",
        "learn.write(\"step,train_a,test_a\\n\" )\n",
        "\n",
        "#学習ループ\n",
        "#配列に入れた加速度データから，ランダムにデータを選択，さらにそこから3x200に切り出して学習にかける\n",
        "for i in range(steps):\n",
        "\n",
        "    #加速度データの中からランダムにデータを選択\n",
        "    batch_mask = np.random.choice(len(train_data),BATCH_SIZE)\n",
        "    tbatch_mask = np.random.choice(len(test_data),BATCH_SIZE)\n",
        "\n",
        "    #バッチ配列\n",
        "    train_data_batch = []\n",
        "    train_label_batch = train_label[batch_mask]\n",
        "    test_data_batch = []\n",
        "\n",
        "    #3x200に切り出し_訓練データ\n",
        "    for g in batch_mask:\n",
        "#         rdata = train_data[g]\n",
        "#         train_data_batch.append(rdata)\n",
        "\n",
        "        b_data = train_data[g]\n",
        "\n",
        "        b_x = b_data[0]\n",
        "        b_y = b_data[1]\n",
        "        b_z = b_data[2]\n",
        "        \n",
        "        rdata = []\n",
        "        start = random.randint(0, len(b_x) - CaptureNumber)\n",
        "        end = start + CaptureNumber\n",
        "\n",
        "        x_data = b_x[start:end]\n",
        "        y_data = b_y[start:end]\n",
        "        z_data = b_z[start:end]\n",
        "\n",
        "        rdata.append(x_data)\n",
        "        rdata.append(y_data)\n",
        "        rdata.append(z_data)\n",
        "\n",
        "        train_data_batch.append(rdata)\n",
        "\n",
        "    # 3x200に切り出し_テストデータ\n",
        "    for g in range(len(test_data)):\n",
        "#         rdata = test_data[g]\n",
        "#         test_data_batch.append(rdata)\n",
        "        b_data = test_data[g]\n",
        "\n",
        "        b_x = b_data[0]\n",
        "        b_y = b_data[1]\n",
        "        b_z = b_data[2]\n",
        "\n",
        "        rdata = []\n",
        "        start = random.randint(0, len(b_x) - CaptureNumber)\n",
        "        end = start + CaptureNumber\n",
        "\n",
        "        x_data = b_x[start:end]\n",
        "        y_data = b_y[start:end]\n",
        "        z_data = b_z[start:end]\n",
        "\n",
        "        rdata.append(x_data)\n",
        "        rdata.append(y_data)\n",
        "        rdata.append(z_data)\n",
        "\n",
        "        test_data_batch.append(rdata)\n",
        "\n",
        "    # 学習の実行　x＝データy＿＝ラベル keep_prob=dropoutでどれだけノード消すか\n",
        "    w_out, _ = sess.run([y_conv, train_step],feed_dict={x: train_data_batch, y_: train_label_batch, keep_prob:drop})\n",
        "\n",
        "    #精度の計算\n",
        "    train_accuracy = accuracy.eval(feed_dict={\n",
        "        x: train_data_batch, y_: train_label_batch, keep_prob:drop})\n",
        "\n",
        "    # 10ステップごとに精度を記録\n",
        "    if i % 10 == 0 :\n",
        "        test_accuracy = accuracy.eval(feed_dict={\n",
        "            x: test_data_batch, y_: test_label, keep_prob: drop})\n",
        "\n",
        "        print(\"step %d, training accuracy %g   test accuracy %g\" % (i, train_accuracy, test_accuracy))\n",
        "        learn.write(\"%d,%g,%g\\n\" % (i, train_accuracy, test_accuracy))\n",
        "\n",
        "    #終了一回前にいろいろ作成\n",
        "    if i == steps-1:\n",
        "\n",
        "        # confusion matrix 作成\n",
        "        y_p = tf.argmax(y_conv, 1)\n",
        "        val_accuracy, y_pred = sess.run([accuracy, y_p], feed_dict={x: test_data_batch, y_: test_label, keep_prob: drop})\n",
        "        y_true = np.argmax(test_label, 1)\n",
        "        con = confusion_matrix(y_true, y_pred)\n",
        "        print(con)\n",
        "\n",
        "        #検証データ分類用\n",
        "        #y_pred_1 = sess.run([y_p],feed_dict={x: Ex_TestDatas, keep_prob: drop})\n",
        "        # y_pred2 = y_pred_1[0]\n",
        "        # print(y_pred2)\n",
        "        # print(len(y_pred2))\n",
        "\n",
        "\n",
        "\n",
        "        print(\"ccn test 正答率 %g\" % accuracy.eval(feed_dict={\n",
        "            x: test_data_batch, y_: test_label, keep_prob: drop\n",
        "        }))\n",
        "\n",
        "#         print(\"ccn realtime test 正答率 %g\" % accuracy.eval(feed_dict={\n",
        "#             x: Ex_TestDatas, y_: Ex_Label, keep_prob: drop\n",
        "#         }))\n",
        "\n",
        "        #処理時間計測，終わり\n",
        "        endTime = datetime.datetime.today()\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"開始時刻 %s\" % (startTime))\n",
        "        print(\"終了時刻 %s\" % (endTime))\n",
        "\n",
        "        learn.write(\"\\n開始時刻 %s\\n\" % (startTime))\n",
        "        learn.write(\"終了時刻 %s\\n\" % (endTime))\n",
        "        learn.write(\"ccn test 正答率 %g\" % accuracy.eval(feed_dict={\n",
        "            x: test_data_batch, y_: test_label, keep_prob: drop\n",
        "        }))\n",
        "\n",
        "        #confusion matrixについて，精度を百分率表記したcon1と何が何回正解したかを表すconの両方つくる\n",
        "        con_sum = np.sum(con, axis=1)\n",
        "        con1 = []\n",
        "        for a in range(len(con)):\n",
        "            c = con[a] / con_sum[a]\n",
        "            con1.append(c)\n",
        "\n",
        "        print(con_sum)\n",
        "        print(con)\n",
        "        result.write(\"Confusion Matrix :\\n\")\n",
        "        # np.savetxt('result.txt',con,fmt=\"%0.2f\")\n",
        "        np.savetxt('confusion-no.csv', con, fmt=\"%0.3f\", delimiter=',')\n",
        "        np.savetxt('confusion.csv', con1, fmt=\"%0.3f\", delimiter=',')\n",
        "\n",
        "\n",
        "        ##学習器の保存処理２\n",
        "saver.save(sess,cwd+\"\\\\0model.ckpt\")\n",
        "\n",
        "\n",
        "\n",
        "learn.close()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DS!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "8\n",
            "9\n",
            "train_folder_name\n",
            "test_folder_name\n",
            ".ds_storeを抹殺\n",
            "8\n",
            "ex.ds_storeを抹殺\n",
            "Ex_Label\n",
            "[1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[131 600 550 217  91 242 296 196 388  13 585 797 683  74  51 653 716 133\n",
            " 779 383 749 587 315  60 380 497 460 313 151 220  81  92 726 455 293  41\n",
            " 361 538 759 660 237 186  99 233 617 208 646 381 222 431  19 736 218 662\n",
            " 545 693 224  97 373 443  39 549 637 789  17 415  49 272 757 471 370  40\n",
            " 275 639 135 783 402 346 722 401 250 570 199  56 701 335 193  50 153 195\n",
            " 144 121 295 673  11 141 760 689 657 413 277 559 725 498 476 365 485 528\n",
            " 225  85  84 741  32 761 464  45 322 687 656 394  52 446 343 219 404  43\n",
            " 113 546 174 565 245  86 630 171 557 504 111 649 780 215 750 521  16 602\n",
            " 558 492 289  96   8 642 730 796  68 316 495 532 169 360 205 426  82 615\n",
            " 633  79 685 254 281  12 677 513 763 392 595 279  47 756 317 239 432 400\n",
            " 419 794 672 799 238  22 137 702 369 774 522 436   1 334 427   0 283  37\n",
            " 311 182  65 132 411 740 732 619  70 188  30 671 330 606 755 209  18 321\n",
            " 260 591 697 241  73 692 230 785 540 708 655 597 482  58 517 438 371 599\n",
            " 149 214  15  94 773 511 393 449 494   5 469 263 428 572 691 700 793 467\n",
            " 525 762 459 578 588 782  46 547 695 325 267 535 124 216 775 364 710 682\n",
            " 154 323 625 539 743 457 801 632 765 307 584 292 581 552 300 728 518 684\n",
            "  44  63 729 786 112 575  55 571  27 248 139 396 544 566 407 128 172 348\n",
            " 663 686 162 103 501 769 778 678  57 596 530 680 453 127   2 787 161 767\n",
            " 102 110 101 509 339 666  48 130 452  80 372 742 592 487  54 661 593 341\n",
            " 508 458 529 734 405 713 226 636 223  10  66 298 291 479 276 733 696 679\n",
            " 374 269 758  33 690 123 122 197 416 598 268 354 390 146 192 201  72 353\n",
            " 706 777 332 483 156 145 688 520 605 626 635 118 231 753 352  28 768 551\n",
            " 157 106 351 568 543 376 312 107 752 164 441 240 117 707 170 641 556 640\n",
            " 784 109 527 624  34 623 270 290 424 744 638 412 579 526  95 440 451 210\n",
            " 709 608  89 766 324 301 435 454 375 177 378 475  36 567 507 190  31 138\n",
            " 531 329 607 668 648 746 667  38 621 410 262 328 614 422 302 705 723 391\n",
            " 299 791 644 202 698 104 163 676 140 731 382 613 478 280 770 399 108 664\n",
            " 739 537 274 357  23 583 515 628 282 331 745 344 389  75 340   3 308 447\n",
            " 246 345 258 658 524 255 359 717  20  14 503  29 737 534 284 490 115  42\n",
            " 439 285 541 631 795 247 764 790 148 629 273 212 243 264 627 134 506 714\n",
            " 349 721 367 514 647 142 450 425 472 523 423 184 160  21 204 576 491 408\n",
            " 253 792 437  25 724 180 493 719 712 305 582 271 116 251 379 179  87 461\n",
            " 665 362 387 417 278 430 610 158 261 347 715 320 798 409 603 776 620 445\n",
            " 168 385 366 748 711 386 480  93  24 652 533 356 516 670 181 155 288 694\n",
            " 536 604 406 297 213  26 207 643 167 342 542  61 309 675 772 333 150 420\n",
            " 165 377 355 754 433 484 266 611 206 470 414 500 703 384 569 287 616 265\n",
            " 232 645 486 244 175  77  67 397 674 114 681 183 314 294 553 429   7 286\n",
            " 659 358  53 304 318  59 548 747 191 303 257 363 560 143   6 609 319  69\n",
            "  83 800 176 561 590 444 738 136 421 194 456 468 704 227 634 368 448 327\n",
            " 512 125 781 119 338 473 200 228 505 618 735 326 462 488 519 699  76 481\n",
            " 147 398 496 129 463  78 126 236 256 159 563 442 189 502 654 198 718 336\n",
            " 187 337 466 580 235 105  98 178 203 477 499 152 173 562 573 586 229 306\n",
            " 211 720 489 574 350 622 788  64 751  90 221 564 669 474 120 395  62 589\n",
            " 166 249 185 100 612 434 650 594 554  35 418 577 259 465 651   4   9 234\n",
            " 771 601 510  71 252 403  88 310 555 727]\n",
            "641\n",
            "160\n",
            "[array([64., 58., 58., ..., 85., 88., 87.]), array([72., 74., 74., ...,  2.,  8., 12.]), array([44., 43., 45., ..., 41., 42., 37.])]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 0, training accuracy 0.373333   test accuracy 0.2\n",
            "step 10, training accuracy 0.59   test accuracy 0.4\n",
            "step 20, training accuracy 0.645   test accuracy 0.6125\n",
            "step 30, training accuracy 0.683333   test accuracy 0.6625\n",
            "step 40, training accuracy 0.703333   test accuracy 0.65\n",
            "step 50, training accuracy 0.733333   test accuracy 0.7125\n",
            "step 60, training accuracy 0.755   test accuracy 0.6875\n",
            "step 70, training accuracy 0.783333   test accuracy 0.70625\n",
            "step 80, training accuracy 0.771667   test accuracy 0.75625\n",
            "step 90, training accuracy 0.806667   test accuracy 0.81875\n",
            "step 100, training accuracy 0.85   test accuracy 0.76875\n",
            "step 110, training accuracy 0.836667   test accuracy 0.78125\n",
            "step 120, training accuracy 0.846667   test accuracy 0.70625\n",
            "step 130, training accuracy 0.83   test accuracy 0.84375\n",
            "step 140, training accuracy 0.87   test accuracy 0.84375\n",
            "step 150, training accuracy 0.856667   test accuracy 0.83125\n",
            "step 160, training accuracy 0.881667   test accuracy 0.83125\n",
            "step 170, training accuracy 0.885   test accuracy 0.85625\n",
            "step 180, training accuracy 0.885   test accuracy 0.8375\n",
            "step 190, training accuracy 0.893333   test accuracy 0.8\n",
            "step 200, training accuracy 0.923333   test accuracy 0.825\n",
            "step 210, training accuracy 0.891667   test accuracy 0.85625\n",
            "step 220, training accuracy 0.89   test accuracy 0.86875\n",
            "step 230, training accuracy 0.881667   test accuracy 0.83125\n",
            "step 240, training accuracy 0.923333   test accuracy 0.88125\n",
            "step 250, training accuracy 0.893333   test accuracy 0.8375\n",
            "step 260, training accuracy 0.933333   test accuracy 0.875\n",
            "step 270, training accuracy 0.921667   test accuracy 0.85\n",
            "step 280, training accuracy 0.921667   test accuracy 0.875\n",
            "step 290, training accuracy 0.936667   test accuracy 0.875\n",
            "step 300, training accuracy 0.926667   test accuracy 0.875\n",
            "step 310, training accuracy 0.928333   test accuracy 0.89375\n",
            "step 320, training accuracy 0.941667   test accuracy 0.86875\n",
            "step 330, training accuracy 0.933333   test accuracy 0.90625\n",
            "step 340, training accuracy 0.941667   test accuracy 0.9125\n",
            "step 350, training accuracy 0.946667   test accuracy 0.89375\n",
            "step 360, training accuracy 0.936667   test accuracy 0.925\n",
            "step 370, training accuracy 0.946667   test accuracy 0.875\n",
            "step 380, training accuracy 0.943333   test accuracy 0.8875\n",
            "step 390, training accuracy 0.935   test accuracy 0.8625\n",
            "step 400, training accuracy 0.941667   test accuracy 0.90625\n",
            "step 410, training accuracy 0.94   test accuracy 0.875\n",
            "step 420, training accuracy 0.96   test accuracy 0.85\n",
            "step 430, training accuracy 0.96   test accuracy 0.90625\n",
            "step 440, training accuracy 0.955   test accuracy 0.8625\n",
            "step 450, training accuracy 0.96   test accuracy 0.86875\n",
            "step 460, training accuracy 0.94   test accuracy 0.875\n",
            "step 470, training accuracy 0.965   test accuracy 0.90625\n",
            "step 480, training accuracy 0.958333   test accuracy 0.86875\n",
            "step 490, training accuracy 0.961667   test accuracy 0.925\n",
            "step 500, training accuracy 0.961667   test accuracy 0.8875\n",
            "step 510, training accuracy 0.955   test accuracy 0.88125\n",
            "step 520, training accuracy 0.975   test accuracy 0.89375\n",
            "step 530, training accuracy 0.953333   test accuracy 0.89375\n",
            "step 540, training accuracy 0.958333   test accuracy 0.8625\n",
            "step 550, training accuracy 0.956667   test accuracy 0.8875\n",
            "step 560, training accuracy 0.956667   test accuracy 0.8875\n",
            "step 570, training accuracy 0.96   test accuracy 0.86875\n",
            "step 580, training accuracy 0.958333   test accuracy 0.875\n",
            "step 590, training accuracy 0.965   test accuracy 0.8875\n",
            "step 600, training accuracy 0.963333   test accuracy 0.90625\n",
            "step 610, training accuracy 0.965   test accuracy 0.85\n",
            "step 620, training accuracy 0.978333   test accuracy 0.9125\n",
            "step 630, training accuracy 0.968333   test accuracy 0.90625\n",
            "step 640, training accuracy 0.976667   test accuracy 0.91875\n",
            "step 650, training accuracy 0.978333   test accuracy 0.8875\n",
            "step 660, training accuracy 0.975   test accuracy 0.9375\n",
            "step 670, training accuracy 0.975   test accuracy 0.89375\n",
            "step 680, training accuracy 0.955   test accuracy 0.91875\n",
            "step 690, training accuracy 0.975   test accuracy 0.94375\n",
            "step 700, training accuracy 0.975   test accuracy 0.925\n",
            "step 710, training accuracy 0.978333   test accuracy 0.8875\n",
            "step 720, training accuracy 0.981667   test accuracy 0.91875\n",
            "step 730, training accuracy 0.968333   test accuracy 0.95625\n",
            "step 740, training accuracy 0.986667   test accuracy 0.88125\n",
            "step 750, training accuracy 0.98   test accuracy 0.9125\n",
            "step 760, training accuracy 0.97   test accuracy 0.91875\n",
            "step 770, training accuracy 0.978333   test accuracy 0.9125\n",
            "step 780, training accuracy 0.981667   test accuracy 0.93125\n",
            "step 790, training accuracy 0.99   test accuracy 0.90625\n",
            "step 800, training accuracy 0.99   test accuracy 0.925\n",
            "step 810, training accuracy 0.988333   test accuracy 0.9375\n",
            "step 820, training accuracy 0.98   test accuracy 0.91875\n",
            "step 830, training accuracy 0.981667   test accuracy 0.88125\n",
            "step 840, training accuracy 0.981667   test accuracy 0.86875\n",
            "step 850, training accuracy 0.98   test accuracy 0.90625\n",
            "step 860, training accuracy 0.991667   test accuracy 0.9\n",
            "step 870, training accuracy 0.985   test accuracy 0.93125\n",
            "step 880, training accuracy 0.98   test accuracy 0.89375\n",
            "step 890, training accuracy 0.988333   test accuracy 0.91875\n",
            "step 900, training accuracy 0.985   test accuracy 0.975\n",
            "step 910, training accuracy 0.983333   test accuracy 0.88125\n",
            "step 920, training accuracy 0.981667   test accuracy 0.89375\n",
            "step 930, training accuracy 0.983333   test accuracy 0.93125\n",
            "step 940, training accuracy 0.986667   test accuracy 0.89375\n",
            "step 950, training accuracy 0.991667   test accuracy 0.91875\n",
            "step 960, training accuracy 0.988333   test accuracy 0.89375\n",
            "step 970, training accuracy 0.991667   test accuracy 0.8875\n",
            "step 980, training accuracy 0.99   test accuracy 0.89375\n",
            "step 990, training accuracy 0.991667   test accuracy 0.8875\n",
            "step 1000, training accuracy 0.99   test accuracy 0.90625\n",
            "step 1010, training accuracy 0.981667   test accuracy 0.93125\n",
            "step 1020, training accuracy 0.991667   test accuracy 0.9125\n",
            "step 1030, training accuracy 0.985   test accuracy 0.9125\n",
            "step 1040, training accuracy 0.99   test accuracy 0.89375\n",
            "step 1050, training accuracy 0.985   test accuracy 0.925\n",
            "step 1060, training accuracy 0.986667   test accuracy 0.93125\n",
            "step 1070, training accuracy 0.988333   test accuracy 0.9\n",
            "step 1080, training accuracy 0.99   test accuracy 0.88125\n",
            "step 1090, training accuracy 0.986667   test accuracy 0.9375\n",
            "step 1100, training accuracy 0.988333   test accuracy 0.89375\n",
            "step 1110, training accuracy 0.995   test accuracy 0.9125\n",
            "step 1120, training accuracy 0.99   test accuracy 0.93125\n",
            "step 1130, training accuracy 0.988333   test accuracy 0.91875\n",
            "step 1140, training accuracy 0.985   test accuracy 0.925\n",
            "step 1150, training accuracy 0.991667   test accuracy 0.95\n",
            "step 1160, training accuracy 0.983333   test accuracy 0.95625\n",
            "step 1170, training accuracy 0.99   test accuracy 0.9125\n",
            "step 1180, training accuracy 0.99   test accuracy 0.90625\n",
            "step 1190, training accuracy 0.991667   test accuracy 0.9375\n",
            "step 1200, training accuracy 0.99   test accuracy 0.90625\n",
            "step 1210, training accuracy 0.986667   test accuracy 0.9375\n",
            "step 1220, training accuracy 0.981667   test accuracy 0.90625\n",
            "step 1230, training accuracy 0.993333   test accuracy 0.925\n",
            "step 1240, training accuracy 1   test accuracy 0.9125\n",
            "step 1250, training accuracy 0.993333   test accuracy 0.9\n",
            "step 1260, training accuracy 0.993333   test accuracy 0.9375\n",
            "step 1270, training accuracy 0.986667   test accuracy 0.9125\n",
            "step 1280, training accuracy 0.988333   test accuracy 0.8875\n",
            "step 1290, training accuracy 0.988333   test accuracy 0.9125\n",
            "step 1300, training accuracy 0.99   test accuracy 0.86875\n",
            "step 1310, training accuracy 0.99   test accuracy 0.95625\n",
            "step 1320, training accuracy 0.995   test accuracy 0.9125\n",
            "step 1330, training accuracy 0.996667   test accuracy 0.95\n",
            "step 1340, training accuracy 0.995   test accuracy 0.9375\n",
            "step 1350, training accuracy 0.988333   test accuracy 0.93125\n",
            "step 1360, training accuracy 0.991667   test accuracy 0.90625\n",
            "step 1370, training accuracy 0.991667   test accuracy 0.8875\n",
            "step 1380, training accuracy 0.993333   test accuracy 0.9375\n",
            "step 1390, training accuracy 0.995   test accuracy 0.93125\n",
            "step 1400, training accuracy 0.991667   test accuracy 0.91875\n",
            "step 1410, training accuracy 0.993333   test accuracy 0.95\n",
            "step 1420, training accuracy 0.995   test accuracy 0.91875\n",
            "step 1430, training accuracy 0.996667   test accuracy 0.91875\n",
            "step 1440, training accuracy 0.995   test accuracy 0.94375\n",
            "step 1450, training accuracy 0.996667   test accuracy 0.9375\n",
            "step 1460, training accuracy 0.996667   test accuracy 0.93125\n",
            "step 1470, training accuracy 0.991667   test accuracy 0.91875\n",
            "step 1480, training accuracy 0.996667   test accuracy 0.94375\n",
            "step 1490, training accuracy 0.995   test accuracy 0.9125\n",
            "step 1500, training accuracy 0.99   test accuracy 0.925\n",
            "step 1510, training accuracy 0.991667   test accuracy 0.9\n",
            "step 1520, training accuracy 1   test accuracy 0.925\n",
            "step 1530, training accuracy 0.991667   test accuracy 0.9125\n",
            "step 1540, training accuracy 0.993333   test accuracy 0.94375\n",
            "step 1550, training accuracy 0.991667   test accuracy 0.90625\n",
            "step 1560, training accuracy 0.985   test accuracy 0.85625\n",
            "step 1570, training accuracy 0.996667   test accuracy 0.93125\n",
            "step 1580, training accuracy 0.993333   test accuracy 0.93125\n",
            "step 1590, training accuracy 0.996667   test accuracy 0.93125\n",
            "step 1600, training accuracy 0.998333   test accuracy 0.9125\n",
            "step 1610, training accuracy 0.998333   test accuracy 0.91875\n",
            "step 1620, training accuracy 0.998333   test accuracy 0.94375\n",
            "step 1630, training accuracy 0.993333   test accuracy 0.9375\n",
            "step 1640, training accuracy 0.995   test accuracy 0.925\n",
            "step 1650, training accuracy 0.996667   test accuracy 0.9625\n",
            "step 1660, training accuracy 0.991667   test accuracy 0.9375\n",
            "step 1670, training accuracy 0.996667   test accuracy 0.9125\n",
            "step 1680, training accuracy 0.998333   test accuracy 0.91875\n",
            "step 1690, training accuracy 0.998333   test accuracy 0.94375\n",
            "step 1700, training accuracy 0.995   test accuracy 0.94375\n",
            "step 1710, training accuracy 0.993333   test accuracy 0.925\n",
            "step 1720, training accuracy 0.99   test accuracy 0.95625\n",
            "step 1730, training accuracy 0.993333   test accuracy 0.95\n",
            "step 1740, training accuracy 0.995   test accuracy 0.9375\n",
            "step 1750, training accuracy 1   test accuracy 0.9375\n",
            "step 1760, training accuracy 0.991667   test accuracy 0.9125\n",
            "step 1770, training accuracy 0.996667   test accuracy 0.88125\n",
            "step 1780, training accuracy 0.998333   test accuracy 0.9125\n",
            "step 1790, training accuracy 0.993333   test accuracy 0.9125\n",
            "step 1800, training accuracy 0.998333   test accuracy 0.95625\n",
            "step 1810, training accuracy 0.991667   test accuracy 0.925\n",
            "step 1820, training accuracy 0.995   test accuracy 0.90625\n",
            "step 1830, training accuracy 0.993333   test accuracy 0.95\n",
            "step 1840, training accuracy 0.998333   test accuracy 0.9375\n",
            "step 1850, training accuracy 0.996667   test accuracy 0.91875\n",
            "step 1860, training accuracy 0.993333   test accuracy 0.9375\n",
            "step 1870, training accuracy 0.995   test accuracy 0.9125\n",
            "step 1880, training accuracy 0.998333   test accuracy 0.90625\n",
            "step 1890, training accuracy 0.996667   test accuracy 0.90625\n",
            "step 1900, training accuracy 0.996667   test accuracy 0.9\n",
            "step 1910, training accuracy 0.995   test accuracy 0.9125\n",
            "step 1920, training accuracy 1   test accuracy 0.89375\n",
            "step 1930, training accuracy 0.996667   test accuracy 0.94375\n",
            "step 1940, training accuracy 0.996667   test accuracy 0.91875\n",
            "step 1950, training accuracy 0.998333   test accuracy 0.94375\n",
            "step 1960, training accuracy 0.995   test accuracy 0.93125\n",
            "step 1970, training accuracy 0.995   test accuracy 0.91875\n",
            "step 1980, training accuracy 1   test accuracy 0.90625\n",
            "step 1990, training accuracy 0.996667   test accuracy 0.9\n",
            "step 2000, training accuracy 1   test accuracy 0.9375\n",
            "step 2010, training accuracy 0.998333   test accuracy 0.90625\n",
            "step 2020, training accuracy 1   test accuracy 0.9375\n",
            "step 2030, training accuracy 0.998333   test accuracy 0.90625\n",
            "step 2040, training accuracy 0.998333   test accuracy 0.90625\n",
            "step 2050, training accuracy 0.996667   test accuracy 0.91875\n",
            "step 2060, training accuracy 0.996667   test accuracy 0.9\n",
            "step 2070, training accuracy 0.998333   test accuracy 0.9125\n",
            "step 2080, training accuracy 0.998333   test accuracy 0.925\n",
            "step 2090, training accuracy 0.998333   test accuracy 0.925\n",
            "step 2100, training accuracy 1   test accuracy 0.95\n",
            "step 2110, training accuracy 0.995   test accuracy 0.925\n",
            "step 2120, training accuracy 0.998333   test accuracy 0.95\n",
            "step 2130, training accuracy 0.996667   test accuracy 0.95\n",
            "step 2140, training accuracy 1   test accuracy 0.9\n",
            "step 2150, training accuracy 0.996667   test accuracy 0.9625\n",
            "step 2160, training accuracy 0.996667   test accuracy 0.91875\n",
            "step 2170, training accuracy 0.996667   test accuracy 0.9375\n",
            "step 2180, training accuracy 1   test accuracy 0.91875\n",
            "step 2190, training accuracy 0.996667   test accuracy 0.90625\n",
            "step 2200, training accuracy 0.995   test accuracy 0.94375\n",
            "step 2210, training accuracy 1   test accuracy 0.925\n",
            "step 2220, training accuracy 1   test accuracy 0.9125\n",
            "step 2230, training accuracy 0.995   test accuracy 0.9375\n",
            "step 2240, training accuracy 1   test accuracy 0.9\n",
            "step 2250, training accuracy 1   test accuracy 0.90625\n",
            "step 2260, training accuracy 0.995   test accuracy 0.93125\n",
            "step 2270, training accuracy 0.995   test accuracy 0.9125\n",
            "step 2280, training accuracy 1   test accuracy 0.93125\n",
            "step 2290, training accuracy 0.998333   test accuracy 0.95\n",
            "step 2300, training accuracy 1   test accuracy 0.9375\n",
            "step 2310, training accuracy 0.996667   test accuracy 0.93125\n",
            "step 2320, training accuracy 1   test accuracy 0.9125\n",
            "step 2330, training accuracy 0.995   test accuracy 0.96875\n",
            "step 2340, training accuracy 0.998333   test accuracy 0.94375\n",
            "step 2350, training accuracy 0.998333   test accuracy 0.89375\n",
            "step 2360, training accuracy 1   test accuracy 0.93125\n",
            "step 2370, training accuracy 1   test accuracy 0.94375\n",
            "step 2380, training accuracy 0.996667   test accuracy 0.94375\n",
            "step 2390, training accuracy 0.998333   test accuracy 0.91875\n",
            "step 2400, training accuracy 0.998333   test accuracy 0.925\n",
            "step 2410, training accuracy 0.998333   test accuracy 0.925\n",
            "step 2420, training accuracy 0.996667   test accuracy 0.93125\n",
            "step 2430, training accuracy 0.995   test accuracy 0.94375\n",
            "step 2440, training accuracy 1   test accuracy 0.94375\n",
            "step 2450, training accuracy 0.995   test accuracy 0.9625\n",
            "step 2460, training accuracy 0.996667   test accuracy 0.9375\n",
            "step 2470, training accuracy 0.996667   test accuracy 0.9125\n",
            "step 2480, training accuracy 0.996667   test accuracy 0.925\n",
            "step 2490, training accuracy 0.996667   test accuracy 0.9\n",
            "step 2500, training accuracy 0.996667   test accuracy 0.93125\n",
            "step 2510, training accuracy 0.998333   test accuracy 0.91875\n",
            "step 2520, training accuracy 0.998333   test accuracy 0.95\n",
            "step 2530, training accuracy 0.996667   test accuracy 0.9125\n",
            "step 2540, training accuracy 0.995   test accuracy 0.91875\n",
            "step 2550, training accuracy 1   test accuracy 0.9625\n",
            "step 2560, training accuracy 0.993333   test accuracy 0.925\n",
            "step 2570, training accuracy 1   test accuracy 0.94375\n",
            "step 2580, training accuracy 1   test accuracy 0.925\n",
            "step 2590, training accuracy 1   test accuracy 0.93125\n",
            "step 2600, training accuracy 0.996667   test accuracy 0.9\n",
            "step 2610, training accuracy 0.998333   test accuracy 0.93125\n",
            "step 2620, training accuracy 0.996667   test accuracy 0.8875\n",
            "step 2630, training accuracy 0.996667   test accuracy 0.925\n",
            "step 2640, training accuracy 0.996667   test accuracy 0.9375\n",
            "step 2650, training accuracy 1   test accuracy 0.925\n",
            "step 2660, training accuracy 0.998333   test accuracy 0.8875\n",
            "step 2670, training accuracy 0.993333   test accuracy 0.91875\n",
            "step 2680, training accuracy 0.998333   test accuracy 0.9\n",
            "step 2690, training accuracy 0.998333   test accuracy 0.9125\n",
            "step 2700, training accuracy 1   test accuracy 0.93125\n",
            "step 2710, training accuracy 0.996667   test accuracy 0.94375\n",
            "step 2720, training accuracy 1   test accuracy 0.8875\n",
            "step 2730, training accuracy 0.998333   test accuracy 0.91875\n",
            "step 2740, training accuracy 0.998333   test accuracy 0.94375\n",
            "step 2750, training accuracy 0.995   test accuracy 0.94375\n",
            "step 2760, training accuracy 1   test accuracy 0.9375\n",
            "step 2770, training accuracy 0.998333   test accuracy 0.9\n",
            "step 2780, training accuracy 0.998333   test accuracy 0.93125\n",
            "step 2790, training accuracy 0.996667   test accuracy 0.90625\n",
            "step 2800, training accuracy 0.995   test accuracy 0.90625\n",
            "step 2810, training accuracy 0.998333   test accuracy 0.9375\n",
            "step 2820, training accuracy 1   test accuracy 0.91875\n",
            "step 2830, training accuracy 0.998333   test accuracy 0.94375\n",
            "step 2840, training accuracy 1   test accuracy 0.9\n",
            "step 2850, training accuracy 0.996667   test accuracy 0.925\n",
            "step 2860, training accuracy 0.998333   test accuracy 0.95625\n",
            "step 2870, training accuracy 0.998333   test accuracy 0.93125\n",
            "step 2880, training accuracy 0.998333   test accuracy 0.9375\n",
            "step 2890, training accuracy 0.998333   test accuracy 0.90625\n",
            "step 2900, training accuracy 0.998333   test accuracy 0.94375\n",
            "step 2910, training accuracy 1   test accuracy 0.95\n",
            "step 2920, training accuracy 0.998333   test accuracy 0.975\n",
            "step 2930, training accuracy 1   test accuracy 0.9375\n",
            "step 2940, training accuracy 0.996667   test accuracy 0.94375\n",
            "step 2950, training accuracy 1   test accuracy 0.93125\n",
            "step 2960, training accuracy 0.998333   test accuracy 0.89375\n",
            "step 2970, training accuracy 0.998333   test accuracy 0.94375\n",
            "step 2980, training accuracy 0.996667   test accuracy 0.9375\n",
            "step 2990, training accuracy 0.995   test accuracy 0.94375\n",
            "[[19  0  0  0  1  0  0  0]\n",
            " [ 0 23  0  0  0  3  0  0]\n",
            " [ 1  0 18  0  0  0  1  0]\n",
            " [ 0  0  0 18  0  0  1  0]\n",
            " [ 0  0  1  0 22  0  1  0]\n",
            " [ 0  2  0  0  0 21  0  0]\n",
            " [ 0  0  1  0  0  0 14  0]\n",
            " [ 0  0  0  0  1  0  0 12]]\n",
            "ccn test 正答率 0.94375\n",
            "\n",
            "開始時刻 2019-02-06 21:35:24.681051\n",
            "終了時刻 2019-02-06 21:49:43.389479\n",
            "[20 26 20 19 24 23 15 13]\n",
            "[[19  0  0  0  1  0  0  0]\n",
            " [ 0 23  0  0  0  3  0  0]\n",
            " [ 1  0 18  0  0  0  1  0]\n",
            " [ 0  0  0 18  0  0  1  0]\n",
            " [ 0  0  1  0 22  0  1  0]\n",
            " [ 0  2  0  0  0 21  0  0]\n",
            " [ 0  0  1  0  0  0 14  0]\n",
            " [ 0  0  0  0  1  0  0 12]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}