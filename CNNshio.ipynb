{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNNshio.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiomasa1218/Colaboratory/blob/master/CNNshio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "u2DXHgAq-QdK",
        "colab_type": "code",
        "outputId": "92f8aa05-e055-43e3-9ebe-0268eb435b77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "trfp-xvj-TgB",
        "colab_type": "code",
        "outputId": "d21da185-fc5f-430e-c53e-2e52ef43c0a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# drive mean root directory of  google drive\n",
        "!ls ./gdrive/'My Drive'/'Kumamoto-Univ'/'Graduationwork'/'exefolder'/'test_folder_name'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n",
            "ls: cannot access './gdrive/My Drive/Kumamoto-Univ/Graduationwork/exefolder/test_folder_name': Transport endpoint is not connected\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Zw_5NFXE-WTR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# check auth\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u2Wf2ubj-atT",
        "colab_type": "code",
        "outputId": "878857e8-c61d-498d-8f6b-dd713aa3fa94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "cd gdrive/'My Drive'/'Kumamoto-Univ'/'Graduationwork'/'exefolder'/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Kumamoto-Univ/Graduationwork/exefolder\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "H9c6rYVo-JKl",
        "colab_type": "code",
        "outputId": "b166453b-c1f6-41f9-c43e-6b5f547ec061",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1222
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import csv\n",
        "import pandas as pd\n",
        "import input_data\n",
        "import time\n",
        "import datetime\n",
        "from sklearn.metrics import confusion_matrix,recall_score,precision_score,f1_score\n",
        "\n",
        "#フィルタ作成\n",
        "def weight_variable(shape):\n",
        "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "    return tf.Variable(initial)\n",
        "     \n",
        "def weight_variable_he(shape,nodes):\n",
        "    initial = tf.truncated_normal(shape, stddev=np.sqrt(2.0/nodes))\n",
        "    return tf.Variable(initial)\n",
        "     \n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.1, shape=shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def conv2d(x,W):\n",
        "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='VALID')\n",
        "\n",
        "def max_pool_2x2(x):\n",
        "    return tf.nn.max_pool(x, ksize = [1,1,2,1], strides=[1,1,2,1],padding = 'VALID')\n",
        "\n",
        "def batch_normalization(shape, input):\n",
        "    eps = 1e-5\n",
        "    gamma = weight_variable([shape])\n",
        "    beta = weight_variable([shape])\n",
        "    mean,variance = tf.nn.moments(input,[0])\n",
        "    return gamma * (input - mean) / tf.sqrt(variance+eps) + beta\n",
        "\n",
        "def Activation(x):\n",
        "    datas = tf.nn.relu(x)\n",
        "    #datas = tf.nn.tanh(x)\n",
        "    return datas\n",
        "\n",
        "\n",
        "\n",
        "#define\n",
        "#Data_Classes = 8#学習クラス数\n",
        "steps = 5000 #エポック数　学習の回数\n",
        "BATCH_SIZE = 1000 #ミニバッチの一回の学習で使う量\n",
        "#TBATCH_SIZE = 10\n",
        "Validation = 0.8  #  ここの値で分割 ex. Validation = 0.4 →　0.4:0.6　に分割\n",
        "CaptureNumber = 200 #ミニバッチの大きさ、初期は２００個のデータ\n",
        "drop = 1.0 #dropoutの係数、NNのノードがこれをかけた数になる\n",
        "\n",
        "allCSize = 4608 #全結合サイズ\n",
        "filterSize = 64 #畳みこみフィルタ（カーネル）数 今回は 1 x filiterSize の大きさ\n",
        "\n",
        "# 訓練データのフォルダのパス\n",
        "train_folder_name = \"train_folder_name\"\n",
        "\n",
        "# 検証データのフォルダのパス\n",
        "test_folder_name = \"test_folder_name\"\n",
        "\n",
        "#全訓練データとそのラベル\n",
        "All_Datas = []\n",
        "All_Label = []\n",
        "\n",
        "#外部検証データとそのラベル\n",
        "Ex_TestDatas=[]\n",
        "Ex_Label = []\n",
        "\n",
        "#並び順をシャッフルしたあとの訓練データ\n",
        "All_SDatas =[]\n",
        "All_SLabel = []\n",
        "\n",
        "#分割後の訓練データ，学習するほう\n",
        "train_data = []\n",
        "train_label = []\n",
        "\n",
        "#分割後の訓練データ，モデル評価するほう\n",
        "test_data = []\n",
        "test_label = []\n",
        "\n",
        "#フォルダ中身のファイル名を取得\n",
        "trainFolder = os.listdir(train_folder_name)\n",
        "testFolder = os.listdir(test_folder_name)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#学習クラス数\n",
        "Data_Classes = len(trainFolder)\n",
        "\n",
        "# .DS_Storeがあるか検知\n",
        "for i,d in enumerate(trainFolder):\n",
        "    if d == \".DS_Store\":\n",
        "        Data_Classes = Data_Classes - 1\n",
        "        print(\"DS!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "\n",
        "print(dsflag)\n",
        "print(Data_Classes)\n",
        "print(len(testFolder))\n",
        "print(train_folder_name)\n",
        "print(test_folder_name)\n",
        "\n",
        "#フォルダごとにみる\n",
        "#訓練データの読み込み\n",
        "\n",
        "dsflag = 0\n",
        "\n",
        "for i,d in enumerate(trainFolder):\n",
        "    #one_hot_vector生成用\n",
        "    tmp = np.zeros(Data_Classes)\n",
        "    if d != \".DS_Store\":\n",
        "        #フォルダ内のファイルのリストを取得\n",
        "        files = os.listdir(train_folder_name + '/'+d)\n",
        "        #print(files)\n",
        "\n",
        "        #one_hot_vectorを作成\n",
        "        tmp[i-dsflag] = 1\n",
        "        #ファイル毎にみる\n",
        "        for f in files:\n",
        "                            \n",
        "            #.DS_Storeをどかす\n",
        "            if f != \".DS_Store\":\n",
        "                #どのファイルを見ているか確認用\n",
        "                #print(\"load:\"+f)\n",
        "\n",
        "                datafile_path = train_folder_name + '/' + d+'/'+f\n",
        "\n",
        "                #csvから読み込み\n",
        "                data = np.loadtxt(datafile_path, delimiter=\",\")     #pbldata用\n",
        "\n",
        "                #3x200のデータに整形\n",
        "                flatdata = []\n",
        "                flatdata.append(data[0])\n",
        "                flatdata.append(data[1])\n",
        "                flatdata.append(data[2])\n",
        "\n",
        "                All_Datas.append(data)\n",
        "                #one_hot_vectorをラベルに追加\n",
        "                All_Label.append(tmp)\n",
        "            else:\n",
        "                print(\".ds_storeを除去\")\n",
        "    else:\n",
        "        print(\".ds_storeを抹殺\")\n",
        "        print(i)\n",
        "        dsflag = dsflag + 1\n",
        "\n",
        "\n",
        "dsflag = 0\n",
        "\n",
        "#検証データの読み込み\n",
        "for i, d in enumerate(testFolder):\n",
        "    #one_hot_vector生成用\n",
        "    tmp = np.zeros(Data_Classes)\n",
        "    if d != \".DS_Store\":\n",
        "        # フォルダ内のファイルのリストを取得\n",
        "        files = os.listdir(test_folder_name + '/' + d)\n",
        "        #print(files)\n",
        "\n",
        "        #one_hot_vectorを作成\n",
        "        tmp[i-dsflag] = 1\n",
        "        #ファイル毎にみる\n",
        "        for f in files:\n",
        "                            \n",
        "            #.DS_Storeをどかす\n",
        "            if f != \".DS_Store\":\n",
        "                #どのファイルを見ているか確認用\n",
        "                #print(\"load:\"+f)\n",
        "\n",
        "                datafile_path = test_folder_name + '/' + d+'/'+f\n",
        "\n",
        "                #csvから読み込み\n",
        "                data = np.loadtxt(datafile_path, delimiter=\",\")     #pbldata用\n",
        "\n",
        "                #3x200のデータに整形\n",
        "                flatdata = []\n",
        "                flatdata.append(data[0, :CaptureNumber])\n",
        "                flatdata.append(data[1, :CaptureNumber])\n",
        "                flatdata.append(data[2, :CaptureNumber])\n",
        "                                     \n",
        "                Ex_TestDatas.append(data)\n",
        "                #one_hot_vectorをラベルに追加\n",
        "                Ex_Label.append(tmp)\n",
        "            else:\n",
        "                print(\".ds_storeを除去\")\n",
        "    else:\n",
        "        print(\"ex.ds_storeを抹殺\")\n",
        "        dsflag = dsflag + 1\n",
        "  \n",
        "print(\"Ex_Label\")\n",
        "print(Ex_Label[0])\n",
        "\n",
        "#訓練データ順列のシャッフル\n",
        "np.random.seed(seed=32)\n",
        "Rindex = np.random.permutation(list(range(len(All_Datas))))\n",
        "print(Rindex)\n",
        "for k in Rindex:\n",
        "    All_SDatas.append(All_Datas[k])\n",
        "    All_SLabel.append(All_Label[k])\n",
        "\n",
        "    \n",
        "#訓練データを学習に使うやつとモデルの評価に使うやつの２種に分ける\n",
        "numberV = int(len(All_SDatas)*(Validation)) #訓練データ数\n",
        "testV = len(All_SDatas) - numberV  #テストデータ数\n",
        "train_data = All_SDatas[:numberV] #0.９まで\n",
        "test_data = All_SDatas[numberV+1:]\n",
        "train_label = All_SLabel[:numberV]\n",
        "test_label = All_SLabel[numberV+1:]\n",
        "\n",
        "print(len(train_data))\n",
        "print(len(test_data))\n",
        "\n",
        "#----------\n",
        "#numpy行列へ\n",
        "#----------\n",
        "\n",
        "train_data = np.array(train_data)\n",
        "train_label = np.array(train_label)\n",
        "\n",
        "test_data = np.array(test_data)\n",
        "test_label = np.array(test_label)\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "x = tf.placeholder('float', shape=[None,3,CaptureNumber]) #3x200のデータ\n",
        "y_ = tf.placeholder('float', shape=[None, Data_Classes]) #正解ラベル\n",
        "keep_prob = tf.placeholder('float')\n",
        "                                     \n",
        "# -----------------------------------------\n",
        "# 畳み込みニューラルネットワーク\n",
        "#\n",
        "# (convconv -> bn -> pool -> drop) x3，\n",
        "# (fc->drop) x2\n",
        "# -----------------------------------------\n",
        "\n",
        "#第1ブロック\n",
        "W_conv1_1 = weight_variable([1,5,3,filterSize]) #1*5のフィルタ　入力３ｃｈ　出力６４枚\n",
        "b_conv1_1 = bias_variable([filterSize]) #出力６４ch\n",
        "x_image = tf.reshape(x,[-1,1,CaptureNumber,3]) #-１はreshapeに適切な数N　１*２００のデータ　３ｃｈ\n",
        "h_conv1_1 = conv2d(x_image, W_conv1_1)+b_conv1_1 \n",
        "tan1_1 = Activation(h_conv1_1)\n",
        "bn1_1 = batch_normalization(filterSize,tan1_1)\n",
        "\n",
        "W_conv1_2 = weight_variable([1,5,filterSize,filterSize])\n",
        "b_conv1_2 = bias_variable([filterSize])\n",
        "h_conv1_2 = conv2d(bn1_1, W_conv1_2)+b_conv1_2\n",
        "tan1_2 = Activation(h_conv1_2)\n",
        "bn1_2 = batch_normalization(filterSize,tan1_2)\n",
        "\n",
        "h_pool1 = max_pool_2x2(bn1_2)\n",
        "h_pool1_drop = tf.nn.dropout(h_pool1, keep_prob)\n",
        "\n",
        "\n",
        "# #第2ブロック\n",
        "W_conv2_1 = weight_variable([1,5,filterSize,filterSize*2])\n",
        "b_conv2_1 = bias_variable([filterSize*2])\n",
        "h_conv2_1 = conv2d(h_pool1_drop, W_conv2_1)+b_conv2_1\n",
        "tan2_1 = Activation(h_conv2_1)\n",
        "bn2_1 = batch_normalization(filterSize*2,tan2_1)\n",
        "\n",
        "W_conv2_2 = weight_variable([1,5,filterSize*2,filterSize*2])\n",
        "b_conv2_2 = bias_variable([filterSize*2])\n",
        "h_conv2_2 = conv2d(bn2_1, W_conv2_2)+b_conv2_2\n",
        "tan2_2 = Activation(h_conv2_2)\n",
        "bn2_2 = batch_normalization(filterSize*2,tan2_2)\n",
        "\n",
        "h_pool2 = max_pool_2x2(bn2_2)\n",
        "h_pool2_drop = tf.nn.dropout(h_pool2, keep_prob)\n",
        "\n",
        "#\n",
        "\n",
        "# #第3ブロック\n",
        "W_conv3_1 = weight_variable([1,5,filterSize*2,filterSize*4])\n",
        "b_conv3_1 = bias_variable([filterSize*4])\n",
        "h_conv3_1 = conv2d(h_pool2_drop, W_conv3_1)+b_conv3_1\n",
        "tan3_1 = Activation(h_conv3_1)\n",
        "bn3_1 = batch_normalization(filterSize*4,tan3_1)\n",
        "\n",
        "W_conv3_2 = weight_variable([1,5,filterSize*4,filterSize*4])\n",
        "b_conv3_2 = bias_variable([filterSize*4])\n",
        "h_conv3_2 = conv2d(bn3_1, W_conv3_2)+b_conv3_2\n",
        "tan3_2 = Activation(h_conv3_2)\n",
        "bn3_2 = batch_normalization(filterSize*4,tan3_2)\n",
        "\n",
        "\n",
        "h_pool3 = max_pool_2x2(bn3_2)\n",
        "h_pool3_drop = tf.nn.dropout(h_pool3, keep_prob)\n",
        "h_pool3_flat = tf.reshape(h_pool3_drop, [-1,allCSize])\n",
        "\n",
        "#全結合層\n",
        "W_fc1 = weight_variable([allCSize, allCSize])\n",
        "b_fc1 = bias_variable([allCSize])\n",
        "h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1)\n",
        "bn4 = batch_normalization(allCSize,h_fc1)\n",
        "\n",
        "# #dropout\n",
        "h_fc1_drop = tf.nn.dropout(bn4, 1.0)\n",
        "\n",
        "# #全結合層\n",
        "W_fc2 = weight_variable([allCSize, allCSize])\n",
        "b_fc2 = bias_variable([allCSize])\n",
        "\n",
        "h_fc2= tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
        "bn5 = batch_normalization(allCSize,h_fc1)\n",
        "\n",
        "# #dropout\n",
        "h_fc2_drop = tf.nn.dropout(bn5, 0.5)\n",
        "\n",
        "#softmaxで出力#\n",
        "W_fc4 = weight_variable([allCSize, Data_Classes])\n",
        "b_fc4 = bias_variable([Data_Classes])\n",
        "y_conv=tf.nn.softmax(tf.matmul(h_fc2_drop, W_fc4) + b_fc4)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#------------------\n",
        "# 評価と損失関数\n",
        "#------------------\n",
        "\n",
        "#L2ノルムを足そうとした残骸\n",
        "# L2_sqr = tf.nn.l2_loss(W_conv1) + tf.nn.l2_loss(W_conv2)+tf.nn.l2_loss(W_conv3) + tf.nn.l2_loss(W_fc1) + tf.nn.l2_loss(W_fc2)\n",
        "# lambda_2 = 0.01\n",
        "\n",
        "#クロスエントロピー誤差関数，1e-7を足して，勾配消失を防止\n",
        "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_*tf.log(y_conv+1e-7),reduction_indices=[1]))\n",
        "#loss = cross_entropy + lambda_2*L2_sqr\n",
        "# cross_entropy = -tf.reduce_sum(y_*tf.log(tf.clip_by_value(y_conv,1e-10,1.0)))\n",
        "\n",
        "#重みの最適化\n",
        "train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\n",
        "#　推定結果のy_convと正解ラベルy_が同じかどうか判定，correct_predictionがbool配列になってる\n",
        "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_,1))\n",
        "# 正答率\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "# run\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "\n",
        "\n",
        "#\n",
        "#学習器の保存処理\n",
        "saver = tf.train.Saver()\n",
        "ckpt = tf.train.get_checkpoint_state('./')\n",
        "cwd = os.getcwd()\n",
        "\n",
        "\n",
        "#処理時間計測\n",
        "startTime = datetime.datetime.today()\n",
        "\n",
        "#ログ\n",
        "learn = open('log.csv','w')\n",
        "result = open('result.txt','w')\n",
        "\n",
        "\n",
        "learn.write(\"class,step\\n\" )\n",
        "learn.write(\"%g,%g\\n\"%(Data_Classes,steps))\n",
        "learn.write(\"step,train_a,test_a\\n\" )\n",
        "\n",
        "#学習ループ\n",
        "#配列に入れた加速度データから，ランダムにデータを選択，さらにそこから3x200に切り出して学習にかける\n",
        "for i in range(steps):\n",
        "\n",
        "    #加速度データの中からランダムにデータを選択\n",
        "    batch_mask = np.random.choice(len(train_data),BATCH_SIZE)\n",
        "    tbatch_mask = np.random.choice(len(test_data),BATCH_SIZE)\n",
        "\n",
        "    #バッチ配列\n",
        "    train_data_batch = []\n",
        "    train_label_batch = train_label[batch_mask]\n",
        "    test_data_batch = []\n",
        "\n",
        "    #3x200に切り出し_訓練データ\n",
        "    for g in batch_mask:\n",
        "        b_data = train_data[g]\n",
        "\n",
        "        b_x = b_data[0]\n",
        "        b_y = b_data[1]\n",
        "        b_z = b_data[2]\n",
        "\n",
        "        rdata = []\n",
        "        start = random.randint(0, len(b_x) - CaptureNumber)\n",
        "        end = start + CaptureNumber\n",
        "\n",
        "        x_data = b_x[start:end]\n",
        "        y_data = b_y[start:end]\n",
        "        z_data = b_z[start:end]\n",
        "\n",
        "        rdata.append(x_data)\n",
        "        rdata.append(y_data)\n",
        "        rdata.append(z_data)\n",
        "\n",
        "        train_data_batch.append(rdata)\n",
        "\n",
        "    # 3x200に切り出し_テストデータ\n",
        "    for g in range(len(test_data)):\n",
        "        b_data = test_data[g]\n",
        "\n",
        "        b_x = b_data[0]\n",
        "        b_y = b_data[1]\n",
        "        b_z = b_data[2]\n",
        "\n",
        "        rdata = []\n",
        "        start = random.randint(0, len(b_x) - CaptureNumber)\n",
        "        end = start + CaptureNumber\n",
        "\n",
        "        x_data = b_x[start:end]\n",
        "        y_data = b_y[start:end]\n",
        "        z_data = b_z[start:end]\n",
        "\n",
        "        rdata.append(x_data)\n",
        "        rdata.append(y_data)\n",
        "        rdata.append(z_data)\n",
        "\n",
        "        test_data_batch.append(rdata)\n",
        "\n",
        "\n",
        "    # 学習の実行　x＝データy＿＝ラベル keep_prob=dropoutでどれだけノード消すか\n",
        "    w_out, _ = sess.run([y_conv, train_step],feed_dict={x: train_data_batch, y_: train_label_batch, keep_prob:drop})\n",
        "\n",
        "    #精度の計算\n",
        "    train_accuracy = accuracy.eval(feed_dict={\n",
        "        x: train_data_batch, y_: train_label_batch, keep_prob:drop})\n",
        "\n",
        "    # 10ステップごとに精度を記録\n",
        "    if i % 10 == 0 :\n",
        "        test_accuracy = accuracy.eval(feed_dict={\n",
        "            x: test_data_batch, y_: test_label, keep_prob: drop})\n",
        "\n",
        "        print(\"step %d, training accuracy %g   test accuracy %g\" % (i, train_accuracy, test_accuracy))\n",
        "        learn.write(\"%d,%g,%g\\n\" % (i, train_accuracy, test_accuracy))\n",
        "\n",
        "    #終了一回前にいろいろ作成\n",
        "    if i == steps-1:\n",
        "\n",
        "        # confusion matrix 作成\n",
        "        y_p = tf.argmax(y_conv, 1)\n",
        "        val_accuracy, y_pred = sess.run([accuracy, y_p], feed_dict={x: test_data_batch, y_: test_label, keep_prob: drop})\n",
        "        y_true = np.argmax(test_label, 1)\n",
        "        con = confusion_matrix(y_true, y_pred)\n",
        "        print(con)\n",
        "\n",
        "        #検証データ分類用\n",
        "        #y_pred_1 = sess.run([y_p],feed_dict={x: Ex_TestDatas, keep_prob: drop})\n",
        "        # y_pred2 = y_pred_1[0]\n",
        "        # print(y_pred2)\n",
        "        # print(len(y_pred2))\n",
        "\n",
        "\n",
        "\n",
        "        print(\"ccn test 正答率 %g\" % accuracy.eval(feed_dict={\n",
        "            x: test_data_batch, y_: test_label, keep_prob: drop\n",
        "        }))\n",
        "\n",
        "#         print(\"ccn realtime test 正答率 %g\" % accuracy.eval(feed_dict={\n",
        "#             x: Ex_TestDatas, y_: Ex_Label, keep_prob: drop\n",
        "#         }))\n",
        "\n",
        "        #処理時間計測，終わり\n",
        "        endTime = datetime.datetime.today()\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"開始時刻 %s\" % (startTime))\n",
        "        print(\"終了時刻 %s\" % (endTime))\n",
        "\n",
        "        learn.write(\"\\n開始時刻 %s\\n\" % (startTime))\n",
        "        learn.write(\"終了時刻 %s\\n\" % (endTime))\n",
        "        learn.write(\"ccn test 正答率 %g\" % accuracy.eval(feed_dict={\n",
        "            x: test_data_batch, y_: test_label, keep_prob: drop\n",
        "        }))\n",
        "\n",
        "        #confusion matrixについて，精度を百分率表記したcon1と何が何回正解したかを表すconの両方つくる\n",
        "        con_sum = np.sum(con, axis=1)\n",
        "        con1 = []\n",
        "        for a in range(len(con)):\n",
        "            c = con[a] / con_sum[a]\n",
        "            con1.append(c)\n",
        "\n",
        "        print(con_sum)\n",
        "        print(con)\n",
        "        result.write(\"Confusion Matrix :\\n\")\n",
        "        # np.savetxt('result.txt',con,fmt=\"%0.2f\")\n",
        "        np.savetxt('confusion-no.csv', con, fmt=\"%0.3f\", delimiter=',')\n",
        "        np.savetxt('confusion.csv', con1, fmt=\"%0.3f\", delimiter=',')\n",
        "\n",
        "\n",
        "        ##学習器の保存処理２\n",
        "saver.save(sess,cwd+\"\\\\0model.ckpt\")\n",
        "\n",
        "\n",
        "\n",
        "learn.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DS!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "1\n",
            "8\n",
            "9\n",
            "train_folder_name\n",
            "test_folder_name\n",
            ".ds_storeを抹殺\n",
            "8\n",
            "ex.ds_storeを抹殺\n",
            "Ex_Label\n",
            "[1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[131 600 550 217  91 242 296 196 388  13 585 797 683  74  51 653 716 133\n",
            " 779 383 749 587 315  60 380 497 460 313 151 220  81  92 726 455 293  41\n",
            " 361 538 759 660 237 186  99 233 617 208 646 381 222 431  19 736 218 662\n",
            " 545 693 224  97 373 443  39 549 637 789  17 415  49 272 757 471 370  40\n",
            " 275 639 135 783 402 346 722 401 250 570 199  56 701 335 193  50 153 195\n",
            " 144 121 295 673  11 141 760 689 657 413 277 559 725 498 476 365 485 528\n",
            " 225  85  84 741  32 761 464  45 322 687 656 394  52 446 343 219 404  43\n",
            " 113 546 174 565 245  86 630 171 557 504 111 649 780 215 750 521  16 602\n",
            " 558 492 289  96   8 642 730 796  68 316 495 532 169 360 205 426  82 615\n",
            " 633  79 685 254 281  12 677 513 763 392 595 279  47 756 317 239 432 400\n",
            " 419 794 672 799 238  22 137 702 369 774 522 436   1 334 427   0 283  37\n",
            " 311 182  65 132 411 740 732 619  70 188  30 671 330 606 755 209  18 321\n",
            " 260 591 697 241  73 692 230 785 540 708 655 597 482  58 517 438 371 599\n",
            " 149 214  15  94 773 511 393 449 494   5 469 263 428 572 691 700 793 467\n",
            " 525 762 459 578 588 782  46 547 695 325 267 535 124 216 775 364 710 682\n",
            " 154 323 625 539 743 457 801 632 765 307 584 292 581 552 300 728 518 684\n",
            "  44  63 729 786 112 575  55 571  27 248 139 396 544 566 407 128 172 348\n",
            " 663 686 162 103 501 769 778 678  57 596 530 680 453 127   2 787 161 767\n",
            " 102 110 101 509 339 666  48 130 452  80 372 742 592 487  54 661 593 341\n",
            " 508 458 529 734 405 713 226 636 223  10  66 298 291 479 276 733 696 679\n",
            " 374 269 758  33 690 123 122 197 416 598 268 354 390 146 192 201  72 353\n",
            " 706 777 332 483 156 145 688 520 605 626 635 118 231 753 352  28 768 551\n",
            " 157 106 351 568 543 376 312 107 752 164 441 240 117 707 170 641 556 640\n",
            " 784 109 527 624  34 623 270 290 424 744 638 412 579 526  95 440 451 210\n",
            " 709 608  89 766 324 301 435 454 375 177 378 475  36 567 507 190  31 138\n",
            " 531 329 607 668 648 746 667  38 621 410 262 328 614 422 302 705 723 391\n",
            " 299 791 644 202 698 104 163 676 140 731 382 613 478 280 770 399 108 664\n",
            " 739 537 274 357  23 583 515 628 282 331 745 344 389  75 340   3 308 447\n",
            " 246 345 258 658 524 255 359 717  20  14 503  29 737 534 284 490 115  42\n",
            " 439 285 541 631 795 247 764 790 148 629 273 212 243 264 627 134 506 714\n",
            " 349 721 367 514 647 142 450 425 472 523 423 184 160  21 204 576 491 408\n",
            " 253 792 437  25 724 180 493 719 712 305 582 271 116 251 379 179  87 461\n",
            " 665 362 387 417 278 430 610 158 261 347 715 320 798 409 603 776 620 445\n",
            " 168 385 366 748 711 386 480  93  24 652 533 356 516 670 181 155 288 694\n",
            " 536 604 406 297 213  26 207 643 167 342 542  61 309 675 772 333 150 420\n",
            " 165 377 355 754 433 484 266 611 206 470 414 500 703 384 569 287 616 265\n",
            " 232 645 486 244 175  77  67 397 674 114 681 183 314 294 553 429   7 286\n",
            " 659 358  53 304 318  59 548 747 191 303 257 363 560 143   6 609 319  69\n",
            "  83 800 176 561 590 444 738 136 421 194 456 468 704 227 634 368 448 327\n",
            " 512 125 781 119 338 473 200 228 505 618 735 326 462 488 519 699  76 481\n",
            " 147 398 496 129 463  78 126 236 256 159 563 442 189 502 654 198 718 336\n",
            " 187 337 466 580 235 105  98 178 203 477 499 152 173 562 573 586 229 306\n",
            " 211 720 489 574 350 622 788  64 751  90 221 564 669 474 120 395  62 589\n",
            " 166 249 185 100 612 434 650 594 554  35 418 577 259 465 651   4   9 234\n",
            " 771 601 510  71 252 403  88 310 555 727]\n",
            "641\n",
            "160\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-bbc48138b0d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;31m#----------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0mtrain_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (4,2444) into shape (4)"
          ]
        }
      ]
    }
  ]
}