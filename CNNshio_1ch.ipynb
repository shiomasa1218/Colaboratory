{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNNshio_1ch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiomasa1218/Colaboratory/blob/master/CNNshio_1ch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "u2DXHgAq-QdK",
        "colab_type": "code",
        "outputId": "39b46c0a-b8b1-4b9d-aa5a-aa0416e865c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "trfp-xvj-TgB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# drive mean root directory of  google drive\n",
        "!ls ./gdrive/'My Drive'/'Kumamoto-Univ'/'Graduationwork'/'exefolder'/'test_folder_name'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zw_5NFXE-WTR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# check auth\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u2Wf2ubj-atT",
        "colab_type": "code",
        "outputId": "8c284982-e8b0-400e-8e1d-95ed80269920",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "cd gdrive/'My Drive'/'Kumamoto-Univ'/'Graduationwork'/'exefolder'/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Kumamoto-Univ/Graduationwork/exefolder\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "H9c6rYVo-JKl",
        "colab_type": "code",
        "outputId": "88500b68-f083-4386-f4f7-d017b26881f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6130
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import csv\n",
        "import pandas as pd\n",
        "import input_data\n",
        "import time\n",
        "import datetime\n",
        "from sklearn.metrics import confusion_matrix,recall_score,precision_score,f1_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "csv_epochs = []\n",
        "csv_loss = []\n",
        "csv_training_loss = []\n",
        "csv_accuracy=[]\n",
        "csv_training_accuracy = []\n",
        "\n",
        "\n",
        "#フィルタ作成\n",
        "def weight_variable(shape):\n",
        "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "    return tf.Variable(initial)\n",
        "     \n",
        "def weight_variable_he(shape,nodes):\n",
        "    initial = tf.truncated_normal(shape, stddev=np.sqrt(2.0/nodes))\n",
        "    return tf.Variable(initial)\n",
        "     \n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.1, shape=shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def conv2d(x,W):\n",
        "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n",
        "\n",
        "def max_pool_1x2(x):\n",
        "    return tf.nn.max_pool(x, ksize = [1,1,2,1], strides=[1,1,2,1],padding = 'SAME')\n",
        "\n",
        "def batch_normalization(shape, input):\n",
        "    eps = 1e-5\n",
        "    gamma = weight_variable([shape])\n",
        "    beta = weight_variable([shape])\n",
        "    mean,variance = tf.nn.moments(input,[0])\n",
        "    return gamma * (input - mean) / tf.sqrt(variance+eps) + beta\n",
        "\n",
        "def Activation(x):\n",
        "    datas = tf.nn.relu(x)\n",
        "    #datas = tf.nn.tanh(x)\n",
        "    return datas\n",
        "\n",
        "\n",
        "\n",
        "#define\n",
        "#Data_Classes = 8#学習クラス数\n",
        "steps = 3000 #エポック数　学習の回数\n",
        "BATCH_SIZE = 600 #ミニバッチの一回の学習で使う量\n",
        "#TBATCH_SIZE = 10\n",
        "Validation = 0.8  #  ここの値で分割 ex. Validation = 0.4 →　0.4:0.6　に分割\n",
        "CaptureNumber = 200 #データ数　ミニバッチの大きさ、初期は２００個のデータ\n",
        "drop = 1.0 #dropoutの係数、NNのノードがこれをかけた数になる\n",
        "\n",
        "allCSize = 6400 #全結合サイズ,一番手前のテンソルのshape[?,1,44,64] → 1*44*64=2816\n",
        "filterSize = 64 #畳みこみフィルタ（カーネル）数 今回は 1 x filiterSize の大きさ\n",
        "\n",
        "# 訓練データのフォルダのパス\n",
        "train_folder_name = \"train_soc321\"\n",
        "\n",
        "# 検証データのフォルダのパス\n",
        "test_folder_name = \"test_folder_name\"\n",
        "\n",
        "#全訓練データとそのラベル\n",
        "All_Datas = []\n",
        "All_Label = []\n",
        "\n",
        "#外部検証データとそのラベル\n",
        "Ex_TestDatas=[]\n",
        "Ex_Label = []\n",
        "\n",
        "#並び順をシャッフルしたあとの訓練データ\n",
        "All_SDatas =[]\n",
        "All_SLabel = []\n",
        "\n",
        "#分割後の訓練データ，学習するほう\n",
        "train_data = []\n",
        "train_label = []\n",
        "\n",
        "#分割後の訓練データ，モデル評価するほう\n",
        "test_data = []\n",
        "test_label = []\n",
        "\n",
        "#フォルダ中身のファイル名を取得\n",
        "trainFolder = os.listdir(train_folder_name)\n",
        "testFolder = os.listdir(test_folder_name)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#学習クラス数\n",
        "Data_Classes = len(trainFolder)\n",
        "\n",
        "# .DS_Storeがあるか検知\n",
        "for i,d in enumerate(trainFolder):\n",
        "    if d == \".DS_Store\":\n",
        "        Data_Classes = Data_Classes - 1\n",
        "        print(\"DS!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "\n",
        "\n",
        "print(Data_Classes)\n",
        "print(len(testFolder))\n",
        "print(train_folder_name)\n",
        "print(test_folder_name)\n",
        "\n",
        "#フォルダごとにみる\n",
        "#訓練データの読み込み\n",
        "\n",
        "dsflag = 0\n",
        "\n",
        "for i,d in enumerate(trainFolder):\n",
        "    #one_hot_vector生成用\n",
        "    tmp = np.zeros(Data_Classes)\n",
        "    if d != \".DS_Store\":\n",
        "        #フォルダ内のファイルのリストを取得\n",
        "        files = os.listdir(train_folder_name + '/'+d)\n",
        "        #print(files)\n",
        "\n",
        "        #one_hot_vectorを作成\n",
        "        tmp[i-dsflag] = 1\n",
        "        #ファイル毎にみる\n",
        "        for f in files:\n",
        "                            \n",
        "            #.DS_Storeをどかす\n",
        "            if f != \".DS_Store\":\n",
        "                #どのファイルを見ているか確認用\n",
        "                #print(\"load:\"+f)\n",
        "\n",
        "                datafile_path = train_folder_name + '/' + d+'/'+f\n",
        "\n",
        "                #csvから読み込み\n",
        "                data = np.loadtxt(datafile_path, delimiter=\",\")     #pbldata用\n",
        "                \n",
        "                \n",
        "                #データ取り出し\n",
        "                csv321 = data[:]\n",
        "             \n",
        "                \n",
        "                flatdata = []\n",
        "                flatdata.append(csv321)\n",
        "          \n",
        "                All_Datas.append(flatdata)\n",
        "                #one_hot_vectorをラベルに追加\n",
        "#                 print(\"load:\"+f+\" ohv: \"+str(tmp))\n",
        "                All_Label.append(tmp)\n",
        "            else:\n",
        "                print(\".ds_storeを除去\")\n",
        "    else:\n",
        "        print(\".ds_storeを抹殺\")\n",
        "        print(i)\n",
        "        dsflag = dsflag + 1\n",
        "\n",
        "\n",
        "dsflag = 0\n",
        "\n",
        "#検証データの読み込み\n",
        "# for i, d in enumerate(testFolder):\n",
        "#     #one_hot_vector生成用\n",
        "#     tmp = np.zeros(Data_Classes)\n",
        "#     if d != \".DS_Store\":\n",
        "#         # フォルダ内のファイルのリストを取得\n",
        "#         files = os.listdir(test_folder_name + '/' + d)\n",
        "#         #print(files)\n",
        "\n",
        "#         #one_hot_vectorを作成\n",
        "#         tmp[i-dsflag] = 1\n",
        "#         #ファイル毎にみる\n",
        "#         for f in files:\n",
        "                            \n",
        "#             #.DS_Storeをどかす\n",
        "#             if f != \".DS_Store\":\n",
        "#                 #どのファイルを見ているか確認用\n",
        "#                 #print(\"load:\"+f)\n",
        "\n",
        "#                 datafile_path = test_folder_name + '/' + d+'/'+f\n",
        "\n",
        "#                 #csvから読み込み\n",
        "#                 data = np.loadtxt(datafile_path, delimiter=\",\")     #pbldata用\n",
        "\n",
        "#                  #3x200のデータに整形\n",
        "#                 x_csv = data[1, :]\n",
        "#                 y_csv = data[2, :]\n",
        "#                 z_csv = data[3, :]\n",
        "                \n",
        "#                 start = random.randint(0, len(x_csv) - CaptureNumber)\n",
        "#                 end = start + CaptureNumber\n",
        "\n",
        "#                 x_data = x_csv[start:end]\n",
        "#                 y_data = y_csv[start:end]\n",
        "#                 z_data = z_csv[start:end]\n",
        "\n",
        "\n",
        "#                 flatdata = []\n",
        "#                 flatdata.append(x_data)\n",
        "#                 flatdata.append(y_data)\n",
        "#                 flatdata.append(z_data)\n",
        "\n",
        "                \n",
        "#                 Ex_TestDatas.append(flatdata)\n",
        "#                 #one_hot_vectorをラベルに追加\n",
        "#                 Ex_Label.append(tmp)\n",
        "#             else:\n",
        "#                 print(\".ds_storeを除去\")\n",
        "#     else:\n",
        "#         print(\"ex.ds_storeを抹殺\")\n",
        "#         dsflag = dsflag + 1\n",
        "  \n",
        "# print(\"Ex_Label\")\n",
        "# print(Ex_Label[0])\n",
        "\n",
        "#訓練データ順列のシャッフル\n",
        "np.random.seed(seed=32)\n",
        "print(len(All_Datas))\n",
        "Rindex = np.random.permutation(list(range(len(All_Datas))))\n",
        "# print(Rindex)\n",
        "for k in Rindex:\n",
        "    All_SDatas.append(All_Datas[k])\n",
        "    All_SLabel.append(All_Label[k])\n",
        "\n",
        "    \n",
        "#訓練データを学習に使うやつとモデルの評価に使うやつの２種に分ける\n",
        "numberV = int(len(All_SDatas)*(Validation)) #訓練データ数\n",
        "testV = len(All_SDatas) - numberV  #テストデータ数\n",
        "train_data = All_SDatas[:numberV] #0.９まで\n",
        "test_data = All_SDatas[numberV+1:]\n",
        "train_label = All_SLabel[:numberV]\n",
        "test_label = All_SLabel[numberV+1:]\n",
        "\n",
        "print(len(train_data))\n",
        "print(len(test_data))\n",
        "\n",
        "print(train_data[0])\n",
        "\n",
        "#----------\n",
        "#numpy行列へ\n",
        "#----------\n",
        "\n",
        "train_data = np.array(train_data)\n",
        "train_label = np.array(train_label)\n",
        "\n",
        "test_data = np.array(test_data)\n",
        "test_label = np.array(test_label)\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "x = tf.placeholder('float', shape=[None,1,CaptureNumber]) #1x200のデータ\n",
        "y_ = tf.placeholder('float', shape=[None, Data_Classes]) #正解ラベル\n",
        "keep_prob = tf.placeholder('float')\n",
        "                                     \n",
        "# -----------------------------------------\n",
        "# 畳み込みニューラルネットワーク\n",
        "#\n",
        "# (convconv -> bn -> pool -> drop) x3，\n",
        "# (fc->drop) x2\n",
        "# -----------------------------------------\n",
        "\n",
        "#第1ブロック\n",
        "W_conv1_1 = weight_variable([1,5,1,filterSize]) #1*5のフィルタ　入力３ｃｈ　出力６４枚\n",
        "b_conv1_1 = bias_variable([filterSize]) #出力６４ch\n",
        "x_image = tf.reshape(x,[-1,1,CaptureNumber,1]) #-１はreshapeに適切な数N　１*２００のデータ　３ｃｈ\n",
        "h_conv1_1 = conv2d(x_image, W_conv1_1)#+b_conv1_1 \n",
        "tan1_1 = Activation(h_conv1_1)\n",
        "bn1_1 = batch_normalization(filterSize,tan1_1)\n",
        "\n",
        "W_conv1_2 = weight_variable([1,5,filterSize,filterSize])\n",
        "b_conv1_2 = bias_variable([filterSize])\n",
        "h_conv1_2 = conv2d(bn1_1, W_conv1_2)#+b_conv1_2\n",
        "tan1_2 = Activation(h_conv1_2)\n",
        "bn1_2 = batch_normalization(filterSize,tan1_2)\n",
        "\n",
        "\n",
        "h_pool1 = max_pool_1x2(bn1_2)\n",
        "h_pool1_drop = tf.nn.dropout(h_pool1, keep_prob)\n",
        "\n",
        "# #第1ブロック\n",
        "# W_conv1_1 = weight_variable([1,5,1,filterSize]) #1*5のフィルタ　入力３ｃｈ　出力６４枚\n",
        "# b_conv1_1 = bias_variable([filterSize]) #出力６４ch\n",
        "# x_image = tf.reshape(x,[-1,1,CaptureNumber,1]) #-１はreshapeに適切な数N　１*２００のデータ　３ｃｈ\n",
        "# h_conv1_1 = conv2d(x_image, W_conv1_1)#+b_conv1_1 \n",
        "# tan1_1 = Activation(h_conv1_1)\n",
        "# bn1_1 = batch_normalization(filterSize,tan1_1)\n",
        "# # print(h_conv1_1.shape)\n",
        "\n",
        "# W_conv1_2 = weight_variable([1,5,filterSize,filterSize])\n",
        "# b_conv1_2 = bias_variable([filterSize])\n",
        "# h_conv1_2 = conv2d(bn1_1, W_conv1_2)#+b_conv1_2\n",
        "# tan1_2 = Activation(h_conv1_2)\n",
        "# bn1_2 = batch_normalization(filterSize,tan1_2)\n",
        "\n",
        "# h_pool1 = max_pool_1x2(bn1_2)\n",
        "# h_pool1_drop = tf.nn.dropout(h_pool1, keep_prob)\n",
        "\n",
        "\n",
        "# # #第2ブロック\n",
        "W_conv2_1 = weight_variable([1,5,filterSize,filterSize*2])\n",
        "b_conv2_1 = bias_variable([filterSize*2])\n",
        "h_conv2_1 = conv2d(h_pool1_drop, W_conv2_1)+b_conv2_1\n",
        "tan2_1 = Activation(h_conv2_1)\n",
        "bn2_1 = batch_normalization(filterSize*2,tan2_1)\n",
        "\n",
        "h_pool2 = max_pool_1x2(bn2_1)\n",
        "h_pool2_drop = tf.nn.dropout(h_pool2, keep_prob)\n",
        "\n",
        "\n",
        "\n",
        "# W_conv2_1 = weight_variable([1,5,filterSize,filterSize*2])\n",
        "# b_conv2_1 = bias_variable([filterSize*2])\n",
        "# h_conv2_1 = conv2d(h_pool1_drop, W_conv2_1)+b_conv2_1\n",
        "# tan2_1 = Activation(h_conv2_1)\n",
        "# bn2_1 = batch_normalization(filterSize*2,tan2_1)\n",
        "\n",
        "# W_conv2_2 = weight_variable([1,5,filterSize*2,filterSize*2])\n",
        "# b_conv2_2 = bias_variable([filterSize*2])\n",
        "# h_conv2_2 = conv2d(bn2_1, W_conv2_2)+b_conv2_2\n",
        "# tan2_2 = Activation(h_conv2_2)\n",
        "# bn2_2 = batch_normalization(filterSize*2,tan2_2)\n",
        "\n",
        "# h_pool2 = max_pool_1x2(bn2_2)\n",
        "# h_pool2_drop = tf.nn.dropout(h_pool2, keep_prob)\n",
        "\n",
        "# # print(h_pool2_drop .shape)\n",
        "\n",
        "# #\n",
        "\n",
        "# # #第3ブロック\n",
        "W_conv3_1 = weight_variable([1,5,filterSize*2,filterSize*4])\n",
        "b_conv3_1 = bias_variable([filterSize*4])\n",
        "h_conv3_1 = conv2d(h_pool2_drop, W_conv3_1)+b_conv3_1\n",
        "tan3_1 = Activation(h_conv3_1)\n",
        "bn3_1 = batch_normalization(filterSize*4,tan3_1)\n",
        "\n",
        "h_pool3 = max_pool_1x2(bn3_1)\n",
        "h_pool3_drop = tf.nn.dropout(h_pool3, keep_prob)\n",
        "\n",
        "# W_conv3_1 = weight_variable([1,5,filterSize*2,filterSize*4])\n",
        "# b_conv3_1 = bias_variable([filterSize*4])\n",
        "# h_conv3_1 = conv2d(h_pool2_drop, W_conv3_1)+b_conv3_1\n",
        "# tan3_1 = Activation(h_conv3_1)\n",
        "# bn3_1 = batch_normalization(filterSize*4,tan3_1)\n",
        "\n",
        "# W_conv3_2 = weight_variable([1,5,filterSize*4,filterSize*4])\n",
        "# b_conv3_2 = bias_variable([filterSize*4])\n",
        "# h_conv3_2 = conv2d(bn3_1, W_conv3_2)+b_conv3_2\n",
        "# tan3_2 = Activation(h_conv3_2)\n",
        "# bn3_2 = batch_normalization(filterSize*4,tan3_2)\n",
        "\n",
        "\n",
        "# h_pool3 = max_pool_1x2(bn3_2)\n",
        "# h_pool3_drop = tf.nn.dropout(h_pool3, keep_prob)\n",
        "print(h_pool3_drop .shape)\n",
        "h_pool3_flat = tf.reshape(h_pool3_drop, [-1,allCSize])\n",
        "\n",
        "# print(h_pool2_drop .shape)\n",
        "# h_pool2_flat = tf.reshape(h_pool2_drop, [-1,allCSize])#\n",
        "\n",
        "#全結合層\n",
        "W_fc1 = weight_variable([allCSize, allCSize])\n",
        "b_fc1 = bias_variable([allCSize])\n",
        "h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1)\n",
        "# h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)#\n",
        "bn4 = batch_normalization(allCSize,h_fc1)\n",
        "\n",
        "# #dropout\n",
        "h_fc1_drop = tf.nn.dropout(bn4, 1.0)\n",
        "\n",
        "# #全結合層\n",
        "W_fc2 = weight_variable([allCSize, allCSize])\n",
        "b_fc2 = bias_variable([allCSize])\n",
        "\n",
        "h_fc2= tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
        "bn5 = batch_normalization(allCSize,h_fc1)\n",
        "\n",
        "# #dropout\n",
        "h_fc2_drop = tf.nn.dropout(bn5, 0.5)\n",
        "\n",
        "#softmaxで出力#\n",
        "W_fc4 = weight_variable([allCSize, Data_Classes])\n",
        "b_fc4 = bias_variable([Data_Classes])\n",
        "y_conv=tf.nn.softmax(tf.matmul(h_fc2_drop, W_fc4) + b_fc4)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#------------------\n",
        "# 評価と損失関数\n",
        "#------------------\n",
        "\n",
        "#L2ノルムを足そうとした残骸\n",
        "L2_sqr = tf.nn.l2_loss(W_conv1_1) + tf.nn.l2_loss(W_conv2_1)+tf.nn.l2_loss(W_conv1_2) + tf.nn.l2_loss(W_fc1) + tf.nn.l2_loss(W_fc2)\n",
        "lambda_2 = 0.001\n",
        "\n",
        "#クロスエントロピー誤差関数，1e-7を足して，勾配消失を防止\n",
        "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_*tf.log(y_conv+1e-7),reduction_indices=[1]))\n",
        "cross_entropy = cross_entropy + lambda_2*L2_sqr\n",
        "# cross_entropy = -tf.reduce_sum(y_*tf.log(tf.clip_by_value(y_conv,1e-10,1.0)))\n",
        "\n",
        "#重みの最適化\n",
        "train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\n",
        "#　推定結果のy_convと正解ラベルy_が同じかどうか判定，correct_predictionがbool配列になってる\n",
        "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_,1))\n",
        "# 正答率\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "# run\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "\n",
        "\n",
        "#\n",
        "#学習器の保存処理\n",
        "saver = tf.train.Saver()\n",
        "ckpt = tf.train.get_checkpoint_state('./')\n",
        "cwd = os.getcwd()\n",
        "\n",
        "\n",
        "#処理時間計測\n",
        "startTime = datetime.datetime.today()\n",
        "\n",
        "#ログ\n",
        "learn = open(train_folder_name+'/'+'log.csv','w')\n",
        "result = open(train_folder_name+'/'+'result.txt','w')\n",
        "\n",
        "\n",
        "learn.write(\"class,step\\n\" )\n",
        "learn.write(\"%g,%g\\n\"%(Data_Classes,steps))\n",
        "learn.write(\"step,train_a,test_a\\n\" )\n",
        "\n",
        "#学習ループ\n",
        "#配列に入れた加速度データから，ランダムにデータを選択，さらにそこから1x200に切り出して学習にかける\n",
        "for i in range(steps):\n",
        "\n",
        "    #加速度データの中からランダムにデータを選択\n",
        "    batch_mask = np.random.choice(len(train_data),BATCH_SIZE)\n",
        "    tbatch_mask = np.random.choice(len(test_data),BATCH_SIZE)\n",
        "\n",
        "    #バッチ配列\n",
        "    train_data_batch = []\n",
        "    train_label_batch = train_label[batch_mask]\n",
        "    test_data_batch = []\n",
        "    \n",
        "#     print(len(batch_mask))\n",
        "    #1x200に切り出し_訓練データ\n",
        "    for g in batch_mask:\n",
        "#         rdata = train_data[g]\n",
        "#         train_data_batch.append(rdata)\n",
        "#         print(train_data[g])\n",
        "        s_data = train_data[g]\n",
        "        b_data = s_data[0]\n",
        "\n",
        "#         print(len(b_data))\n",
        "        rdata = []\n",
        "        start = random.randint(0, len(b_data) - CaptureNumber)\n",
        "        end = start + CaptureNumber\n",
        "\n",
        "        x_data = b_data[start:end]\n",
        "      \n",
        "        rdata.append(x_data)\n",
        "        train_data_batch.append(rdata)\n",
        "\n",
        "    # 1x200に切り出し_テストデータ\n",
        "    for g in range(len(test_data)):\n",
        "#         rdata = test_data[g]\n",
        "#         test_data_batch.append(rdata)\n",
        "        b_data = test_data[g]\n",
        "  \n",
        "\n",
        "        b_x = b_data[0]\n",
        "        rdata = []\n",
        "        start = random.randint(0, len(b_x) - CaptureNumber)\n",
        "        end = start + CaptureNumber\n",
        "\n",
        "        x_data = b_x[start:end]\n",
        "        rdata.append(x_data)\n",
        "        test_data_batch.append(rdata)\n",
        "\n",
        "    # 学習の実行　x＝データy＿＝ラベル keep_prob=dropoutでどれだけノード消すか\n",
        "    w_out, _ = sess.run([y_conv, train_step],feed_dict={x: train_data_batch, y_: train_label_batch, keep_prob:drop})\n",
        "\n",
        "    #精度の計算\n",
        "    train_accuracy = accuracy.eval(feed_dict={\n",
        "        x: train_data_batch, y_: train_label_batch, keep_prob:drop})\n",
        "    csv_epochs.append(i)\n",
        "\n",
        "    training_loss = cross_entropy.eval(feed_dict={\n",
        "        x: train_data_batch, y_: train_label_batch, keep_prob:drop})\n",
        "    csv_training_loss.append(training_loss)\n",
        "\n",
        "    test_accuracy = accuracy.eval(feed_dict={\n",
        "        x: test_data_batch, y_: test_label, keep_prob: drop})\n",
        "\n",
        "    loss = cross_entropy.eval(feed_dict={\n",
        "        x: test_data_batch, y_: test_label, keep_prob: drop})\n",
        "    csv_loss.append(loss)\n",
        "\n",
        "    csv_accuracy.append(test_accuracy)\n",
        "    csv_training_accuracy.append(train_accuracy)\n",
        "    \n",
        "    # 10ステップごとに精度を記録\n",
        "    if i % 10 == 0 :\n",
        "        # 各epochで下記のようにその時のepochとlossなどを追加していく\n",
        "        \n",
        "\n",
        "        print(\"step %d, training accuracy %g   test accuracy %g   training_loss %g test_loss %g\" % (i, train_accuracy, test_accuracy,training_loss, loss))\n",
        "        learn.write(\"%d,%g,%g\\n\" % (i, train_accuracy, test_accuracy))\n",
        "\n",
        "    #終了一回前にいろいろ作成\n",
        "    if i == steps-1:\n",
        "\n",
        "        # confusion matrix 作成\n",
        "        y_p = tf.argmax(y_conv, 1)\n",
        "        val_accuracy, y_pred = sess.run([accuracy, y_p], feed_dict={x: test_data_batch, y_: test_label, keep_prob: drop})\n",
        "        y_true = np.argmax(test_label, 1)\n",
        "        con = confusion_matrix(y_true, y_pred)\n",
        "        print(con)\n",
        "\n",
        "        #検証データ分類用\n",
        "        #y_pred_1 = sess.run([y_p],feed_dict={x: Ex_TestDatas, keep_prob: drop})\n",
        "        # y_pred2 = y_pred_1[0]\n",
        "        # print(y_pred2)\n",
        "        # print(len(y_pred2))\n",
        "\n",
        "\n",
        "\n",
        "        print(\"ccn test 正答率 %g\" % accuracy.eval(feed_dict={\n",
        "            x: test_data_batch, y_: test_label, keep_prob: drop\n",
        "        }))\n",
        "\n",
        "#         print(\"ccn realtime test 正答率 %g\" % accuracy.eval(feed_dict={\n",
        "#             x: Ex_TestDatas, y_: Ex_Label, keep_prob: drop\n",
        "#         }))\n",
        "\n",
        "        #処理時間計測，終わり\n",
        "        endTime = datetime.datetime.today()\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"開始時刻 %s\" % (startTime))\n",
        "        print(\"終了時刻 %s\" % (endTime))\n",
        "\n",
        "        learn.write(\"\\n開始時刻 %s\\n\" % (startTime))\n",
        "        learn.write(\"終了時刻 %s\\n\" % (endTime))\n",
        "        learn.write(\"ccn test 正答率 %g\" % accuracy.eval(feed_dict={\n",
        "            x: test_data_batch, y_: test_label, keep_prob: drop\n",
        "        }))\n",
        "\n",
        "        #confusion matrixについて，精度を百分率表記したcon1と何が何回正解したかを表すconの両方つくる\n",
        "        con_sum = np.sum(con, axis=1)\n",
        "        con1 = []\n",
        "        for a in range(len(con)):\n",
        "            c = con[a] / con_sum[a]\n",
        "            con1.append(c)\n",
        "\n",
        "        print(con_sum)\n",
        "        print(con)\n",
        "        result.write(\"Confusion Matrix :\\n\")\n",
        "        # np.savetxt('result.txt',con,fmt=\"%0.2f\")\n",
        "        np.savetxt(train_folder_name+'/'+'confusion-no.csv', con, fmt=\"%0.3f\", delimiter=',')\n",
        "        np.savetxt(train_folder_name+'/'+'confusion.csv', con1, fmt=\"%0.3f\", delimiter=',')\n",
        "\n",
        "\n",
        "        ##学習器の保存処理２\n",
        "saver.save(sess,cwd+\"\\\\0model.ckpt\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 計算中あるいは計算終了時にpngグラフを作成\n",
        "plt.plot(csv_epochs, csv_training_loss, \"b\", label=\"Training Loss\")\n",
        "plt.plot(csv_epochs, csv_loss, \"r\", label=\"Test Loss\")\n",
        "plt.vlines(csv_epochs[-1], 0, csv_loss[-1], color='0.75')\n",
        "plt.hlines(csv_loss[-1], 0, csv_epochs[-1], color='0.75')\n",
        "# plt.savefig(\"graphs/model_name.png\")\n",
        "\n",
        "# plt.plot(csv_epochs, csv_training_accuracy, \"b\", label=\"Training Accuracy\")\n",
        "# plt.plot(csv_epochs, csv_accuracy, \"r\", label=\"Test Accuracy\")\n",
        "# plt.vlines(csv_epochs[-1], 0, csv_accuracy[-1], color='0.75')\n",
        "# plt.hlines(csv_accuracy[-1], 0, csv_epochs[-1], color='0.75')\n",
        "# show()\n",
        "# plt.savefig(\"graphs/model_name.png\")\n",
        "\n",
        "\n",
        "learn.close()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n",
            "1\n",
            "train_soc321\n",
            "test_folder_name\n",
            "802\n",
            "641\n",
            "160\n",
            "[array([103., 110., 124., ..., 174., 178., 179.])]\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-5-756cf7b02f74>:280: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "(?, 1, 25, 256)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "step 0, training accuracy 0.403333   test accuracy 0.2   training_loss 313.552 test_loss 313.975\n",
            "step 10, training accuracy 0.508333   test accuracy 0.45625   training_loss 263.887 test_loss 263.998\n",
            "step 20, training accuracy 0.536667   test accuracy 0.51875   training_loss 220.694 test_loss 220.88\n",
            "step 30, training accuracy 0.598333   test accuracy 0.56875   training_loss 184.097 test_loss 183.944\n",
            "step 40, training accuracy 0.636667   test accuracy 0.6375   training_loss 153.237 test_loss 153.314\n",
            "step 50, training accuracy 0.68   test accuracy 0.6375   training_loss 127.471 test_loss 127.576\n",
            "step 60, training accuracy 0.673333   test accuracy 0.61875   training_loss 106.144 test_loss 106.375\n",
            "step 70, training accuracy 0.72   test accuracy 0.6   training_loss 88.3058 test_loss 88.4639\n",
            "step 80, training accuracy 0.708333   test accuracy 0.63125   training_loss 73.5286 test_loss 73.5896\n",
            "step 90, training accuracy 0.71   test accuracy 0.61875   training_loss 61.1606 test_loss 61.4604\n",
            "step 100, training accuracy 0.758333   test accuracy 0.69375   training_loss 50.918 test_loss 50.8826\n",
            "step 110, training accuracy 0.778333   test accuracy 0.71875   training_loss 42.38 test_loss 42.3652\n",
            "step 120, training accuracy 0.795   test accuracy 0.725   training_loss 35.2422 test_loss 35.3478\n",
            "step 130, training accuracy 0.78   test accuracy 0.775   training_loss 29.2945 test_loss 29.5148\n",
            "step 140, training accuracy 0.776667   test accuracy 0.71875   training_loss 24.479 test_loss 24.6805\n",
            "step 150, training accuracy 0.793333   test accuracy 0.7   training_loss 20.4266 test_loss 20.6165\n",
            "step 160, training accuracy 0.815   test accuracy 0.74375   training_loss 16.994 test_loss 17.2873\n",
            "step 170, training accuracy 0.833333   test accuracy 0.8   training_loss 14.225 test_loss 14.3958\n",
            "step 180, training accuracy 0.81   test accuracy 0.74375   training_loss 11.9434 test_loss 12.1286\n",
            "step 190, training accuracy 0.83   test accuracy 0.7625   training_loss 9.98928 test_loss 10.2407\n",
            "step 200, training accuracy 0.826667   test accuracy 0.7   training_loss 8.5001 test_loss 8.77714\n",
            "step 210, training accuracy 0.838333   test accuracy 0.78125   training_loss 7.22671 test_loss 7.35084\n",
            "step 220, training accuracy 0.838333   test accuracy 0.7625   training_loss 6.10852 test_loss 6.29123\n",
            "step 230, training accuracy 0.855   test accuracy 0.775   training_loss 5.17133 test_loss 5.39237\n",
            "step 240, training accuracy 0.856667   test accuracy 0.825   training_loss 4.47191 test_loss 4.55277\n",
            "step 250, training accuracy 0.866667   test accuracy 0.7625   training_loss 3.88932 test_loss 4.15642\n",
            "step 260, training accuracy 0.846667   test accuracy 0.84375   training_loss 3.42468 test_loss 3.47457\n",
            "step 270, training accuracy 0.836667   test accuracy 0.78125   training_loss 3.02684 test_loss 3.09156\n",
            "step 280, training accuracy 0.838333   test accuracy 0.75   training_loss 2.68308 test_loss 2.81674\n",
            "step 290, training accuracy 0.856667   test accuracy 0.775   training_loss 2.3086 test_loss 2.62697\n",
            "step 300, training accuracy 0.88   test accuracy 0.74375   training_loss 2.06283 test_loss 2.45079\n",
            "step 310, training accuracy 0.858333   test accuracy 0.8375   training_loss 1.91209 test_loss 2.03674\n",
            "step 320, training accuracy 0.853333   test accuracy 0.825   training_loss 1.76573 test_loss 1.94163\n",
            "step 330, training accuracy 0.875   test accuracy 0.7875   training_loss 1.61156 test_loss 1.85457\n",
            "step 340, training accuracy 0.886667   test accuracy 0.81875   training_loss 1.54239 test_loss 1.75666\n",
            "step 350, training accuracy 0.858333   test accuracy 0.79375   training_loss 1.40029 test_loss 1.57177\n",
            "step 360, training accuracy 0.87   test accuracy 0.84375   training_loss 1.33195 test_loss 1.50168\n",
            "step 370, training accuracy 0.898333   test accuracy 0.7625   training_loss 1.20072 test_loss 1.65349\n",
            "step 380, training accuracy 0.868333   test accuracy 0.83125   training_loss 1.18692 test_loss 1.43347\n",
            "step 390, training accuracy 0.878333   test accuracy 0.75   training_loss 1.12948 test_loss 1.41985\n",
            "step 400, training accuracy 0.893333   test accuracy 0.825   training_loss 1.08087 test_loss 1.2855\n",
            "step 410, training accuracy 0.891667   test accuracy 0.78125   training_loss 1.01733 test_loss 1.32524\n",
            "step 420, training accuracy 0.88   test accuracy 0.775   training_loss 1.00854 test_loss 1.33146\n",
            "step 430, training accuracy 0.88   test accuracy 0.825   training_loss 1.01144 test_loss 1.16811\n",
            "step 440, training accuracy 0.896667   test accuracy 0.79375   training_loss 0.939782 test_loss 1.26606\n",
            "step 450, training accuracy 0.891667   test accuracy 0.8125   training_loss 0.908916 test_loss 1.05457\n",
            "step 460, training accuracy 0.893333   test accuracy 0.8125   training_loss 0.885213 test_loss 1.19919\n",
            "step 470, training accuracy 0.891667   test accuracy 0.81875   training_loss 0.876569 test_loss 1.07469\n",
            "step 480, training accuracy 0.903333   test accuracy 0.79375   training_loss 0.803718 test_loss 1.13811\n",
            "step 490, training accuracy 0.92   test accuracy 0.8   training_loss 0.76366 test_loss 1.16076\n",
            "step 500, training accuracy 0.903333   test accuracy 0.8125   training_loss 0.796767 test_loss 1.07757\n",
            "step 510, training accuracy 0.9   test accuracy 0.825   training_loss 0.738567 test_loss 1.16675\n",
            "step 520, training accuracy 0.925   test accuracy 0.81875   training_loss 0.702221 test_loss 1.00392\n",
            "step 530, training accuracy 0.895   test accuracy 0.825   training_loss 0.786928 test_loss 1.01742\n",
            "step 540, training accuracy 0.885   test accuracy 0.8375   training_loss 0.791158 test_loss 0.930857\n",
            "step 550, training accuracy 0.903333   test accuracy 0.8375   training_loss 0.741054 test_loss 0.931461\n",
            "step 560, training accuracy 0.898333   test accuracy 0.8625   training_loss 0.75362 test_loss 0.981938\n",
            "step 570, training accuracy 0.91   test accuracy 0.8625   training_loss 0.710285 test_loss 0.887284\n",
            "step 580, training accuracy 0.915   test accuracy 0.8125   training_loss 0.694209 test_loss 1.00575\n",
            "step 590, training accuracy 0.936667   test accuracy 0.8125   training_loss 0.667681 test_loss 0.970378\n",
            "step 600, training accuracy 0.906667   test accuracy 0.83125   training_loss 0.730311 test_loss 0.963271\n",
            "step 610, training accuracy 0.903333   test accuracy 0.85   training_loss 0.698263 test_loss 0.91356\n",
            "step 620, training accuracy 0.916667   test accuracy 0.79375   training_loss 0.688581 test_loss 1.07321\n",
            "step 630, training accuracy 0.928333   test accuracy 0.80625   training_loss 0.640292 test_loss 0.933204\n",
            "step 640, training accuracy 0.916667   test accuracy 0.7625   training_loss 0.656618 test_loss 1.06544\n",
            "step 650, training accuracy 0.916667   test accuracy 0.8625   training_loss 0.652253 test_loss 0.802615\n",
            "step 660, training accuracy 0.926667   test accuracy 0.83125   training_loss 0.646397 test_loss 0.904797\n",
            "step 670, training accuracy 0.915   test accuracy 0.8125   training_loss 0.652361 test_loss 0.879634\n",
            "step 680, training accuracy 0.926667   test accuracy 0.775   training_loss 0.610647 test_loss 1.03498\n",
            "step 690, training accuracy 0.931667   test accuracy 0.85   training_loss 0.609316 test_loss 0.869106\n",
            "step 700, training accuracy 0.918333   test accuracy 0.85625   training_loss 0.666183 test_loss 0.887031\n",
            "step 710, training accuracy 0.923333   test accuracy 0.825   training_loss 0.586922 test_loss 0.889742\n",
            "step 720, training accuracy 0.926667   test accuracy 0.81875   training_loss 0.607571 test_loss 0.882859\n",
            "step 730, training accuracy 0.926667   test accuracy 0.78125   training_loss 0.584815 test_loss 0.94893\n",
            "step 740, training accuracy 0.943333   test accuracy 0.825   training_loss 0.555076 test_loss 0.934257\n",
            "step 750, training accuracy 0.908333   test accuracy 0.8375   training_loss 0.621903 test_loss 0.906676\n",
            "step 760, training accuracy 0.901667   test accuracy 0.80625   training_loss 0.646542 test_loss 0.901184\n",
            "step 770, training accuracy 0.93   test accuracy 0.84375   training_loss 0.560467 test_loss 0.885926\n",
            "step 780, training accuracy 0.92   test accuracy 0.8   training_loss 0.57882 test_loss 0.89862\n",
            "step 790, training accuracy 0.928333   test accuracy 0.8375   training_loss 0.593363 test_loss 0.804365\n",
            "step 800, training accuracy 0.92   test accuracy 0.8375   training_loss 0.594073 test_loss 0.915367\n",
            "step 810, training accuracy 0.93   test accuracy 0.78125   training_loss 0.547425 test_loss 0.924891\n",
            "step 820, training accuracy 0.91   test accuracy 0.8125   training_loss 0.591445 test_loss 1.0282\n",
            "step 830, training accuracy 0.938333   test accuracy 0.85625   training_loss 0.5476 test_loss 0.798969\n",
            "step 840, training accuracy 0.925   test accuracy 0.85625   training_loss 0.550245 test_loss 0.774618\n",
            "step 850, training accuracy 0.945   test accuracy 0.89375   training_loss 0.511126 test_loss 0.715224\n",
            "step 860, training accuracy 0.936667   test accuracy 0.8375   training_loss 0.55995 test_loss 0.777962\n",
            "step 870, training accuracy 0.936667   test accuracy 0.8375   training_loss 0.546891 test_loss 0.829113\n",
            "step 880, training accuracy 0.938333   test accuracy 0.83125   training_loss 0.520528 test_loss 0.776451\n",
            "step 890, training accuracy 0.943333   test accuracy 0.84375   training_loss 0.535942 test_loss 0.793548\n",
            "step 900, training accuracy 0.945   test accuracy 0.84375   training_loss 0.522199 test_loss 0.795914\n",
            "step 910, training accuracy 0.935   test accuracy 0.85   training_loss 0.545156 test_loss 0.797536\n",
            "step 920, training accuracy 0.93   test accuracy 0.8625   training_loss 0.548304 test_loss 0.875193\n",
            "step 930, training accuracy 0.938333   test accuracy 0.79375   training_loss 0.519248 test_loss 0.895976\n",
            "step 940, training accuracy 0.931667   test accuracy 0.8625   training_loss 0.538023 test_loss 0.733806\n",
            "step 950, training accuracy 0.955   test accuracy 0.8625   training_loss 0.494265 test_loss 0.790528\n",
            "step 960, training accuracy 0.945   test accuracy 0.825   training_loss 0.484733 test_loss 0.787221\n",
            "step 970, training accuracy 0.946667   test accuracy 0.80625   training_loss 0.485084 test_loss 0.834732\n",
            "step 980, training accuracy 0.956667   test accuracy 0.85   training_loss 0.508524 test_loss 0.779171\n",
            "step 990, training accuracy 0.95   test accuracy 0.8125   training_loss 0.504479 test_loss 0.848475\n",
            "step 1000, training accuracy 0.936667   test accuracy 0.81875   training_loss 0.527142 test_loss 0.855812\n",
            "step 1010, training accuracy 0.936667   test accuracy 0.83125   training_loss 0.500134 test_loss 0.845186\n",
            "step 1020, training accuracy 0.953333   test accuracy 0.85625   training_loss 0.50476 test_loss 0.775957\n",
            "step 1030, training accuracy 0.96   test accuracy 0.85   training_loss 0.457041 test_loss 0.707955\n",
            "step 1040, training accuracy 0.941667   test accuracy 0.825   training_loss 0.477534 test_loss 0.791186\n",
            "step 1050, training accuracy 0.948333   test accuracy 0.8   training_loss 0.502845 test_loss 0.917875\n",
            "step 1060, training accuracy 0.931667   test accuracy 0.8125   training_loss 0.523187 test_loss 0.740564\n",
            "step 1070, training accuracy 0.94   test accuracy 0.80625   training_loss 0.513273 test_loss 0.931548\n",
            "step 1080, training accuracy 0.946667   test accuracy 0.83125   training_loss 0.499001 test_loss 0.803462\n",
            "step 1090, training accuracy 0.933333   test accuracy 0.84375   training_loss 0.524744 test_loss 0.831858\n",
            "step 1100, training accuracy 0.938333   test accuracy 0.79375   training_loss 0.519375 test_loss 0.875352\n",
            "step 1110, training accuracy 0.961667   test accuracy 0.8375   training_loss 0.458283 test_loss 0.740531\n",
            "step 1120, training accuracy 0.958333   test accuracy 0.83125   training_loss 0.467047 test_loss 0.915786\n",
            "step 1130, training accuracy 0.958333   test accuracy 0.8625   training_loss 0.447915 test_loss 0.821365\n",
            "step 1140, training accuracy 0.948333   test accuracy 0.83125   training_loss 0.477159 test_loss 0.885777\n",
            "step 1150, training accuracy 0.955   test accuracy 0.8375   training_loss 0.461581 test_loss 0.798354\n",
            "step 1160, training accuracy 0.951667   test accuracy 0.85625   training_loss 0.467334 test_loss 0.791006\n",
            "step 1170, training accuracy 0.953333   test accuracy 0.81875   training_loss 0.436181 test_loss 0.979451\n",
            "step 1180, training accuracy 0.956667   test accuracy 0.89375   training_loss 0.472492 test_loss 0.800687\n",
            "step 1190, training accuracy 0.95   test accuracy 0.86875   training_loss 0.456938 test_loss 0.65347\n",
            "step 1200, training accuracy 0.931667   test accuracy 0.8375   training_loss 0.5208 test_loss 0.780815\n",
            "step 1210, training accuracy 0.948333   test accuracy 0.85   training_loss 0.466981 test_loss 0.733084\n",
            "step 1220, training accuracy 0.956667   test accuracy 0.86875   training_loss 0.457023 test_loss 0.780947\n",
            "step 1230, training accuracy 0.945   test accuracy 0.88125   training_loss 0.472233 test_loss 0.665298\n",
            "step 1240, training accuracy 0.938333   test accuracy 0.85   training_loss 0.501355 test_loss 0.807804\n",
            "step 1250, training accuracy 0.951667   test accuracy 0.8625   training_loss 0.460145 test_loss 0.876185\n",
            "step 1260, training accuracy 0.941667   test accuracy 0.825   training_loss 0.467689 test_loss 0.880861\n",
            "step 1270, training accuracy 0.941667   test accuracy 0.85   training_loss 0.466284 test_loss 0.853208\n",
            "step 1280, training accuracy 0.931667   test accuracy 0.81875   training_loss 0.475176 test_loss 0.859799\n",
            "step 1290, training accuracy 0.956667   test accuracy 0.8875   training_loss 0.445146 test_loss 0.755077\n",
            "step 1300, training accuracy 0.956667   test accuracy 0.825   training_loss 0.429107 test_loss 0.783721\n",
            "step 1310, training accuracy 0.95   test accuracy 0.875   training_loss 0.453242 test_loss 0.672509\n",
            "step 1320, training accuracy 0.95   test accuracy 0.8375   training_loss 0.459793 test_loss 0.735954\n",
            "step 1330, training accuracy 0.953333   test accuracy 0.85   training_loss 0.440171 test_loss 0.849983\n",
            "step 1340, training accuracy 0.956667   test accuracy 0.80625   training_loss 0.438859 test_loss 0.867831\n",
            "step 1350, training accuracy 0.951667   test accuracy 0.88125   training_loss 0.465007 test_loss 0.785089\n",
            "step 1360, training accuracy 0.975   test accuracy 0.8375   training_loss 0.411604 test_loss 0.816837\n",
            "step 1370, training accuracy 0.95   test accuracy 0.8625   training_loss 0.417968 test_loss 0.713239\n",
            "step 1380, training accuracy 0.96   test accuracy 0.8125   training_loss 0.418187 test_loss 0.917241\n",
            "step 1390, training accuracy 0.958333   test accuracy 0.85   training_loss 0.419162 test_loss 0.864519\n",
            "step 1400, training accuracy 0.936667   test accuracy 0.83125   training_loss 0.440379 test_loss 0.837905\n",
            "step 1410, training accuracy 0.958333   test accuracy 0.8125   training_loss 0.416963 test_loss 0.815496\n",
            "step 1420, training accuracy 0.956667   test accuracy 0.85   training_loss 0.43416 test_loss 0.708489\n",
            "step 1430, training accuracy 0.961667   test accuracy 0.85   training_loss 0.423742 test_loss 0.766244\n",
            "step 1440, training accuracy 0.953333   test accuracy 0.85625   training_loss 0.444202 test_loss 0.842313\n",
            "step 1450, training accuracy 0.94   test accuracy 0.79375   training_loss 0.435438 test_loss 0.968754\n",
            "step 1460, training accuracy 0.965   test accuracy 0.875   training_loss 0.438711 test_loss 0.644221\n",
            "step 1470, training accuracy 0.955   test accuracy 0.83125   training_loss 0.433514 test_loss 0.861199\n",
            "step 1480, training accuracy 0.97   test accuracy 0.88125   training_loss 0.403303 test_loss 0.645153\n",
            "step 1490, training accuracy 0.961667   test accuracy 0.8125   training_loss 0.405936 test_loss 0.91356\n",
            "step 1500, training accuracy 0.951667   test accuracy 0.8375   training_loss 0.440013 test_loss 0.868527\n",
            "step 1510, training accuracy 0.951667   test accuracy 0.84375   training_loss 0.441325 test_loss 0.824226\n",
            "step 1520, training accuracy 0.955   test accuracy 0.89375   training_loss 0.421132 test_loss 0.702668\n",
            "step 1530, training accuracy 0.956667   test accuracy 0.85   training_loss 0.418687 test_loss 0.746843\n",
            "step 1540, training accuracy 0.958333   test accuracy 0.825   training_loss 0.429085 test_loss 0.777204\n",
            "step 1550, training accuracy 0.96   test accuracy 0.825   training_loss 0.406699 test_loss 0.851013\n",
            "step 1560, training accuracy 0.956667   test accuracy 0.84375   training_loss 0.414139 test_loss 0.759405\n",
            "step 1570, training accuracy 0.948333   test accuracy 0.83125   training_loss 0.438056 test_loss 0.852953\n",
            "step 1580, training accuracy 0.941667   test accuracy 0.8625   training_loss 0.459893 test_loss 0.670166\n",
            "step 1590, training accuracy 0.953333   test accuracy 0.81875   training_loss 0.430951 test_loss 0.912582\n",
            "step 1600, training accuracy 0.953333   test accuracy 0.89375   training_loss 0.443391 test_loss 0.61721\n",
            "step 1610, training accuracy 0.953333   test accuracy 0.86875   training_loss 0.403093 test_loss 0.744674\n",
            "step 1620, training accuracy 0.965   test accuracy 0.85   training_loss 0.403634 test_loss 0.816304\n",
            "step 1630, training accuracy 0.958333   test accuracy 0.825   training_loss 0.408388 test_loss 0.86133\n",
            "step 1640, training accuracy 0.951667   test accuracy 0.84375   training_loss 0.420817 test_loss 0.741838\n",
            "step 1650, training accuracy 0.951667   test accuracy 0.825   training_loss 0.424997 test_loss 0.786955\n",
            "step 1660, training accuracy 0.96   test accuracy 0.85   training_loss 0.389973 test_loss 0.729903\n",
            "step 1670, training accuracy 0.97   test accuracy 0.8375   training_loss 0.372453 test_loss 0.923853\n",
            "step 1680, training accuracy 0.955   test accuracy 0.80625   training_loss 0.424402 test_loss 0.787122\n",
            "step 1690, training accuracy 0.956667   test accuracy 0.8375   training_loss 0.388817 test_loss 0.912868\n",
            "step 1700, training accuracy 0.953333   test accuracy 0.825   training_loss 0.420614 test_loss 0.834949\n",
            "step 1710, training accuracy 0.956667   test accuracy 0.8375   training_loss 0.442446 test_loss 0.786851\n",
            "step 1720, training accuracy 0.958333   test accuracy 0.825   training_loss 0.395204 test_loss 0.854011\n",
            "step 1730, training accuracy 0.96   test accuracy 0.85625   training_loss 0.397782 test_loss 0.892472\n",
            "step 1740, training accuracy 0.951667   test accuracy 0.85   training_loss 0.423648 test_loss 0.759362\n",
            "step 1750, training accuracy 0.953333   test accuracy 0.88125   training_loss 0.399441 test_loss 0.670709\n",
            "step 1760, training accuracy 0.956667   test accuracy 0.7875   training_loss 0.382887 test_loss 0.962439\n",
            "step 1770, training accuracy 0.958333   test accuracy 0.825   training_loss 0.393056 test_loss 0.862156\n",
            "step 1780, training accuracy 0.951667   test accuracy 0.825   training_loss 0.426132 test_loss 0.84221\n",
            "step 1790, training accuracy 0.953333   test accuracy 0.85625   training_loss 0.400457 test_loss 0.837603\n",
            "step 1800, training accuracy 0.965   test accuracy 0.85625   training_loss 0.386516 test_loss 0.76852\n",
            "step 1810, training accuracy 0.966667   test accuracy 0.825   training_loss 0.374765 test_loss 0.740924\n",
            "step 1820, training accuracy 0.963333   test accuracy 0.89375   training_loss 0.414987 test_loss 0.608189\n",
            "step 1830, training accuracy 0.958333   test accuracy 0.825   training_loss 0.415122 test_loss 0.820992\n",
            "step 1840, training accuracy 0.966667   test accuracy 0.8   training_loss 0.39778 test_loss 0.955946\n",
            "step 1850, training accuracy 0.958333   test accuracy 0.85625   training_loss 0.38101 test_loss 0.919069\n",
            "step 1860, training accuracy 0.96   test accuracy 0.85625   training_loss 0.395897 test_loss 0.695096\n",
            "step 1870, training accuracy 0.953333   test accuracy 0.88125   training_loss 0.398959 test_loss 0.702858\n",
            "step 1880, training accuracy 0.956667   test accuracy 0.88125   training_loss 0.386138 test_loss 0.742093\n",
            "step 1890, training accuracy 0.963333   test accuracy 0.8375   training_loss 0.384309 test_loss 0.985221\n",
            "step 1900, training accuracy 0.971667   test accuracy 0.8375   training_loss 0.367267 test_loss 0.726727\n",
            "step 1910, training accuracy 0.96   test accuracy 0.80625   training_loss 0.362696 test_loss 0.850096\n",
            "step 1920, training accuracy 0.963333   test accuracy 0.825   training_loss 0.392327 test_loss 0.847807\n",
            "step 1930, training accuracy 0.97   test accuracy 0.85   training_loss 0.36445 test_loss 0.753527\n",
            "step 1940, training accuracy 0.956667   test accuracy 0.8125   training_loss 0.375904 test_loss 0.894335\n",
            "step 1950, training accuracy 0.961667   test accuracy 0.83125   training_loss 0.366379 test_loss 0.783808\n",
            "step 1960, training accuracy 0.966667   test accuracy 0.84375   training_loss 0.398889 test_loss 0.71793\n",
            "step 1970, training accuracy 0.946667   test accuracy 0.8375   training_loss 0.383349 test_loss 0.721167\n",
            "step 1980, training accuracy 0.955   test accuracy 0.83125   training_loss 0.379146 test_loss 0.856488\n",
            "step 1990, training accuracy 0.97   test accuracy 0.775   training_loss 0.37946 test_loss 0.976619\n",
            "step 2000, training accuracy 0.975   test accuracy 0.85625   training_loss 0.362364 test_loss 0.734072\n",
            "step 2010, training accuracy 0.955   test accuracy 0.85625   training_loss 0.380282 test_loss 0.67869\n",
            "step 2020, training accuracy 0.963333   test accuracy 0.84375   training_loss 0.377459 test_loss 0.717985\n",
            "step 2030, training accuracy 0.96   test accuracy 0.8875   training_loss 0.375811 test_loss 0.692041\n",
            "step 2040, training accuracy 0.948333   test accuracy 0.84375   training_loss 0.385643 test_loss 0.943604\n",
            "step 2050, training accuracy 0.956667   test accuracy 0.8375   training_loss 0.39169 test_loss 0.796982\n",
            "step 2060, training accuracy 0.958333   test accuracy 0.89375   training_loss 0.398018 test_loss 0.718788\n",
            "step 2070, training accuracy 0.956667   test accuracy 0.825   training_loss 0.368281 test_loss 0.781716\n",
            "step 2080, training accuracy 0.966667   test accuracy 0.84375   training_loss 0.370762 test_loss 0.812773\n",
            "step 2090, training accuracy 0.961667   test accuracy 0.8625   training_loss 0.391494 test_loss 0.710979\n",
            "step 2100, training accuracy 0.97   test accuracy 0.85   training_loss 0.358599 test_loss 0.813197\n",
            "step 2110, training accuracy 0.971667   test accuracy 0.8625   training_loss 0.366311 test_loss 0.827578\n",
            "step 2120, training accuracy 0.965   test accuracy 0.86875   training_loss 0.364813 test_loss 0.906001\n",
            "step 2130, training accuracy 0.965   test accuracy 0.8375   training_loss 0.380084 test_loss 0.759974\n",
            "step 2140, training accuracy 0.953333   test accuracy 0.84375   training_loss 0.365603 test_loss 0.847536\n",
            "step 2150, training accuracy 0.976667   test accuracy 0.8375   training_loss 0.340257 test_loss 0.768061\n",
            "step 2160, training accuracy 0.956667   test accuracy 0.83125   training_loss 0.372068 test_loss 0.761098\n",
            "step 2170, training accuracy 0.96   test accuracy 0.8375   training_loss 0.375886 test_loss 0.757273\n",
            "step 2180, training accuracy 0.963333   test accuracy 0.85   training_loss 0.354585 test_loss 0.809136\n",
            "step 2190, training accuracy 0.963333   test accuracy 0.8   training_loss 0.333912 test_loss 0.897299\n",
            "step 2200, training accuracy 0.963333   test accuracy 0.8625   training_loss 0.364659 test_loss 0.678049\n",
            "step 2210, training accuracy 0.965   test accuracy 0.875   training_loss 0.361184 test_loss 0.658045\n",
            "step 2220, training accuracy 0.961667   test accuracy 0.8375   training_loss 0.372133 test_loss 0.73477\n",
            "step 2230, training accuracy 0.971667   test accuracy 0.83125   training_loss 0.340176 test_loss 0.823716\n",
            "step 2240, training accuracy 0.975   test accuracy 0.84375   training_loss 0.344482 test_loss 0.813262\n",
            "step 2250, training accuracy 0.97   test accuracy 0.8375   training_loss 0.367906 test_loss 0.79839\n",
            "step 2260, training accuracy 0.971667   test accuracy 0.84375   training_loss 0.337856 test_loss 0.713333\n",
            "step 2270, training accuracy 0.965   test accuracy 0.85625   training_loss 0.369042 test_loss 0.869257\n",
            "step 2280, training accuracy 0.965   test accuracy 0.825   training_loss 0.350254 test_loss 0.866291\n",
            "step 2290, training accuracy 0.981667   test accuracy 0.875   training_loss 0.347497 test_loss 0.685334\n",
            "step 2300, training accuracy 0.951667   test accuracy 0.85   training_loss 0.391429 test_loss 0.890236\n",
            "step 2310, training accuracy 0.976667   test accuracy 0.8625   training_loss 0.338695 test_loss 0.704389\n",
            "step 2320, training accuracy 0.971667   test accuracy 0.875   training_loss 0.353276 test_loss 0.629136\n",
            "step 2330, training accuracy 0.958333   test accuracy 0.84375   training_loss 0.367143 test_loss 0.763514\n",
            "step 2340, training accuracy 0.97   test accuracy 0.88125   training_loss 0.363082 test_loss 0.69701\n",
            "step 2350, training accuracy 0.96   test accuracy 0.825   training_loss 0.368429 test_loss 0.82304\n",
            "step 2360, training accuracy 0.973333   test accuracy 0.86875   training_loss 0.349301 test_loss 0.811978\n",
            "step 2370, training accuracy 0.965   test accuracy 0.80625   training_loss 0.365498 test_loss 0.884631\n",
            "step 2380, training accuracy 0.96   test accuracy 0.88125   training_loss 0.351183 test_loss 0.711304\n",
            "step 2390, training accuracy 0.976667   test accuracy 0.825   training_loss 0.338414 test_loss 0.926566\n",
            "step 2400, training accuracy 0.963333   test accuracy 0.84375   training_loss 0.366467 test_loss 0.70074\n",
            "step 2410, training accuracy 0.961667   test accuracy 0.85625   training_loss 0.36818 test_loss 0.611493\n",
            "step 2420, training accuracy 0.958333   test accuracy 0.84375   training_loss 0.365432 test_loss 0.709377\n",
            "step 2430, training accuracy 0.958333   test accuracy 0.88125   training_loss 0.355614 test_loss 0.581086\n",
            "step 2440, training accuracy 0.96   test accuracy 0.83125   training_loss 0.351456 test_loss 0.769013\n",
            "step 2450, training accuracy 0.966667   test accuracy 0.8375   training_loss 0.35464 test_loss 0.940019\n",
            "step 2460, training accuracy 0.958333   test accuracy 0.825   training_loss 0.370618 test_loss 0.810874\n",
            "step 2470, training accuracy 0.98   test accuracy 0.80625   training_loss 0.309241 test_loss 0.760818\n",
            "step 2480, training accuracy 0.965   test accuracy 0.8   training_loss 0.347359 test_loss 0.904738\n",
            "step 2490, training accuracy 0.966667   test accuracy 0.88125   training_loss 0.324524 test_loss 0.678313\n",
            "step 2500, training accuracy 0.958333   test accuracy 0.8375   training_loss 0.36233 test_loss 0.777504\n",
            "step 2510, training accuracy 0.965   test accuracy 0.825   training_loss 0.334632 test_loss 0.765219\n",
            "step 2520, training accuracy 0.981667   test accuracy 0.8875   training_loss 0.31966 test_loss 0.634822\n",
            "step 2530, training accuracy 0.963333   test accuracy 0.85   training_loss 0.348479 test_loss 0.920305\n",
            "step 2540, training accuracy 0.973333   test accuracy 0.875   training_loss 0.33478 test_loss 0.767889\n",
            "step 2550, training accuracy 0.973333   test accuracy 0.79375   training_loss 0.322017 test_loss 0.77845\n",
            "step 2560, training accuracy 0.968333   test accuracy 0.81875   training_loss 0.329616 test_loss 0.977285\n",
            "step 2570, training accuracy 0.971667   test accuracy 0.85625   training_loss 0.32648 test_loss 0.927209\n",
            "step 2580, training accuracy 0.98   test accuracy 0.81875   training_loss 0.314597 test_loss 0.802587\n",
            "step 2590, training accuracy 0.97   test accuracy 0.8   training_loss 0.345468 test_loss 0.787498\n",
            "step 2600, training accuracy 0.973333   test accuracy 0.8375   training_loss 0.32942 test_loss 0.849564\n",
            "step 2610, training accuracy 0.973333   test accuracy 0.85   training_loss 0.321188 test_loss 0.68308\n",
            "step 2620, training accuracy 0.96   test accuracy 0.8625   training_loss 0.350647 test_loss 0.707098\n",
            "step 2630, training accuracy 0.968333   test accuracy 0.85   training_loss 0.335928 test_loss 0.697177\n",
            "step 2640, training accuracy 0.976667   test accuracy 0.85   training_loss 0.31286 test_loss 0.889378\n",
            "step 2650, training accuracy 0.973333   test accuracy 0.89375   training_loss 0.311996 test_loss 0.637357\n",
            "step 2660, training accuracy 0.976667   test accuracy 0.86875   training_loss 0.345455 test_loss 0.739661\n",
            "step 2670, training accuracy 0.966667   test accuracy 0.8125   training_loss 0.346669 test_loss 0.989909\n",
            "step 2680, training accuracy 0.968333   test accuracy 0.85625   training_loss 0.33807 test_loss 0.741544\n",
            "step 2690, training accuracy 0.961667   test accuracy 0.8625   training_loss 0.361196 test_loss 0.826452\n",
            "step 2700, training accuracy 0.971667   test accuracy 0.875   training_loss 0.35606 test_loss 0.649001\n",
            "step 2710, training accuracy 0.96   test accuracy 0.78125   training_loss 0.357705 test_loss 1.03227\n",
            "step 2720, training accuracy 0.971667   test accuracy 0.8875   training_loss 0.344357 test_loss 0.762878\n",
            "step 2730, training accuracy 0.983333   test accuracy 0.83125   training_loss 0.320468 test_loss 1.00019\n",
            "step 2740, training accuracy 0.975   test accuracy 0.8625   training_loss 0.340525 test_loss 0.787958\n",
            "step 2750, training accuracy 0.961667   test accuracy 0.84375   training_loss 0.338164 test_loss 0.708779\n",
            "step 2760, training accuracy 0.973333   test accuracy 0.89375   training_loss 0.328843 test_loss 0.616079\n",
            "step 2770, training accuracy 0.965   test accuracy 0.86875   training_loss 0.329419 test_loss 0.728175\n",
            "step 2780, training accuracy 0.973333   test accuracy 0.86875   training_loss 0.30841 test_loss 0.654358\n",
            "step 2790, training accuracy 0.978333   test accuracy 0.83125   training_loss 0.316396 test_loss 0.829505\n",
            "step 2800, training accuracy 0.976667   test accuracy 0.84375   training_loss 0.321365 test_loss 0.717401\n",
            "step 2810, training accuracy 0.97   test accuracy 0.85625   training_loss 0.323057 test_loss 0.86001\n",
            "step 2820, training accuracy 0.97   test accuracy 0.79375   training_loss 0.328857 test_loss 0.820118\n",
            "step 2830, training accuracy 0.973333   test accuracy 0.8875   training_loss 0.325819 test_loss 0.79088\n",
            "step 2840, training accuracy 0.973333   test accuracy 0.875   training_loss 0.322079 test_loss 0.774741\n",
            "step 2850, training accuracy 0.976667   test accuracy 0.8375   training_loss 0.323755 test_loss 0.860186\n",
            "step 2860, training accuracy 0.973333   test accuracy 0.83125   training_loss 0.308299 test_loss 0.858431\n",
            "step 2870, training accuracy 0.966667   test accuracy 0.85   training_loss 0.332053 test_loss 0.610609\n",
            "step 2880, training accuracy 0.973333   test accuracy 0.8375   training_loss 0.340664 test_loss 0.791871\n",
            "step 2890, training accuracy 0.976667   test accuracy 0.81875   training_loss 0.331399 test_loss 0.953139\n",
            "step 2900, training accuracy 0.978333   test accuracy 0.84375   training_loss 0.324072 test_loss 0.655767\n",
            "step 2910, training accuracy 0.966667   test accuracy 0.85   training_loss 0.310563 test_loss 0.740912\n",
            "step 2920, training accuracy 0.98   test accuracy 0.83125   training_loss 0.316352 test_loss 0.770311\n",
            "step 2930, training accuracy 0.968333   test accuracy 0.875   training_loss 0.319952 test_loss 0.68398\n",
            "step 2940, training accuracy 0.976667   test accuracy 0.84375   training_loss 0.315539 test_loss 0.669244\n",
            "step 2950, training accuracy 0.981667   test accuracy 0.85   training_loss 0.314864 test_loss 0.786226\n",
            "step 2960, training accuracy 0.971667   test accuracy 0.9   training_loss 0.327689 test_loss 0.745804\n",
            "step 2970, training accuracy 0.976667   test accuracy 0.8875   training_loss 0.304789 test_loss 0.710383\n",
            "step 2980, training accuracy 0.978333   test accuracy 0.8   training_loss 0.324485 test_loss 0.877118\n",
            "step 2990, training accuracy 0.978333   test accuracy 0.84375   training_loss 0.312737 test_loss 0.735355\n",
            "[[14  0  0  0  4  0  0  0]\n",
            " [ 1 22  0  1  0  1  1  1]\n",
            " [ 0  0 18  1  0  1  0  0]\n",
            " [ 0  0  0 20  0  0  0  0]\n",
            " [ 2  0  0  0 22  0  0  0]\n",
            " [ 0  1  2  0  1 17  2  0]\n",
            " [ 0  0  0  0  0  1 14  0]\n",
            " [ 0  0  0  0  0  0  0 13]]\n",
            "ccn test 正答率 0.8625\n",
            "\n",
            "開始時刻 2019-02-10 10:19:20.283304\n",
            "終了時刻 2019-02-10 10:39:34.179140\n",
            "[18 27 20 20 24 23 15 13]\n",
            "[[14  0  0  0  4  0  0  0]\n",
            " [ 1 22  0  1  0  1  1  1]\n",
            " [ 0  0 18  1  0  1  0  0]\n",
            " [ 0  0  0 20  0  0  0  0]\n",
            " [ 2  0  0  0 22  0  0  0]\n",
            " [ 0  1  2  0  1 17  2  0]\n",
            " [ 0  0  0  0  0  1 14  0]\n",
            " [ 0  0  0  0  0  0  0 13]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAFKCAYAAADMuCxnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "5WQ8FJcntvG9",
        "colab_type": "code",
        "outputId": "93c83726-b4e4-4e13-9f7b-add0a8874e96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "cell_type": "code",
      "source": [
        "plt.plot(csv_epochs, csv_training_loss, \"b\", label=\"Training Loss\")\n",
        "plt.plot(csv_epochs, csv_loss, \"r\", label=\"Test Loss\")\n",
        "plt.xlim([0,steps])\n",
        "plt.ylim([0,2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfIAAAFOCAYAAAB9mZ/eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}