{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNNshio_1ch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiomasa1218/Colaboratory/blob/master/CNNshio_1ch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "u2DXHgAq-QdK",
        "colab_type": "code",
        "outputId": "bfae50cc-3a54-4109-fd85-1a1edbc1df44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "trfp-xvj-TgB",
        "colab_type": "code",
        "outputId": "c08c756d-3a51-4460-b4f6-3cfbe43d9b56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# drive mean root directory of  google drive\n",
        "!ls ./gdrive/'My Drive'/'Kumamoto-Univ'/'Graduationwork'/'exefolder'/'test_folder_name'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "carpet1_2.0_afterlinear   sponge-y_2.0_afterlinear\n",
            "carpet2_2.0_afterlinear   stonetile1_2.0_afterlinear\n",
            "carpet3_2.0_afterlinear   whiteitile1_2.0_afterlinear\n",
            "sponge-g_2.0_afterlinear  woodtile1_2.0_afterlinear\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Zw_5NFXE-WTR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# check auth\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u2Wf2ubj-atT",
        "colab_type": "code",
        "outputId": "72da617d-0248-4177-d976-3caade024920",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "cd gdrive/'My Drive'/'Kumamoto-Univ'/'Graduationwork'/'exefolder'/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Kumamoto-Univ/Graduationwork/exefolder\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "H9c6rYVo-JKl",
        "colab_type": "code",
        "outputId": "bb1bce10-0693-4739-c459-a602f7534aa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import csv\n",
        "import pandas as pd\n",
        "import input_data\n",
        "import time\n",
        "import datetime\n",
        "from sklearn.metrics import confusion_matrix,recall_score,precision_score,f1_score\n",
        "\n",
        "#フィルタ作成\n",
        "def weight_variable(shape):\n",
        "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "    return tf.Variable(initial)\n",
        "     \n",
        "def weight_variable_he(shape,nodes):\n",
        "    initial = tf.truncated_normal(shape, stddev=np.sqrt(2.0/nodes))\n",
        "    return tf.Variable(initial)\n",
        "     \n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.1, shape=shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def conv2d(x,W):\n",
        "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='VALID')\n",
        "\n",
        "def max_pool_1x2(x):\n",
        "    return tf.nn.max_pool(x, ksize = [1,1,2,1], strides=[1,1,2,1],padding = 'VALID')\n",
        "\n",
        "def batch_normalization(shape, input):\n",
        "    eps = 1e-5\n",
        "    gamma = weight_variable([shape])\n",
        "    beta = weight_variable([shape])\n",
        "    mean,variance = tf.nn.moments(input,[0])\n",
        "    return gamma * (input - mean) / tf.sqrt(variance+eps) + beta\n",
        "\n",
        "def Activation(x):\n",
        "    datas = tf.nn.relu(x)\n",
        "    #datas = tf.nn.tanh(x)\n",
        "    return datas\n",
        "\n",
        "\n",
        "\n",
        "#define\n",
        "#Data_Classes = 8#学習クラス数\n",
        "steps = 3000 #エポック数　学習の回数\n",
        "BATCH_SIZE = 600 #ミニバッチの一回の学習で使う量\n",
        "#TBATCH_SIZE = 10\n",
        "Validation = 0.8  #  ここの値で分割 ex. Validation = 0.4 →　0.4:0.6　に分割\n",
        "CaptureNumber = 200 #データ数　ミニバッチの大きさ、初期は２００個のデータ\n",
        "drop = 1.0 #dropoutの係数、NNのノードがこれをかけた数になる\n",
        "\n",
        "allCSize = 9216 #全結合サイズ,一番手前のテンソルのshape[?,1,44,64] → 1*44*64=2816\n",
        "filterSize = 128 #畳みこみフィルタ（カーネル）数 今回は 1 x filiterSize の大きさ\n",
        "\n",
        "# 訓練データのフォルダのパス\n",
        "train_folder_name = \"train_dft321\"\n",
        "\n",
        "# 検証データのフォルダのパス\n",
        "test_folder_name = \"test_folder_name\"\n",
        "\n",
        "#全訓練データとそのラベル\n",
        "All_Datas = []\n",
        "All_Label = []\n",
        "\n",
        "#外部検証データとそのラベル\n",
        "Ex_TestDatas=[]\n",
        "Ex_Label = []\n",
        "\n",
        "#並び順をシャッフルしたあとの訓練データ\n",
        "All_SDatas =[]\n",
        "All_SLabel = []\n",
        "\n",
        "#分割後の訓練データ，学習するほう\n",
        "train_data = []\n",
        "train_label = []\n",
        "\n",
        "#分割後の訓練データ，モデル評価するほう\n",
        "test_data = []\n",
        "test_label = []\n",
        "\n",
        "#フォルダ中身のファイル名を取得\n",
        "trainFolder = os.listdir(train_folder_name)\n",
        "testFolder = os.listdir(test_folder_name)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#学習クラス数\n",
        "Data_Classes = len(trainFolder)\n",
        "\n",
        "# .DS_Storeがあるか検知\n",
        "for i,d in enumerate(trainFolder):\n",
        "    if d == \".DS_Store\":\n",
        "        Data_Classes = Data_Classes - 1\n",
        "        print(\"DS!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "\n",
        "\n",
        "print(Data_Classes)\n",
        "print(len(testFolder))\n",
        "print(train_folder_name)\n",
        "print(test_folder_name)\n",
        "\n",
        "#フォルダごとにみる\n",
        "#訓練データの読み込み\n",
        "\n",
        "dsflag = 0\n",
        "\n",
        "for i,d in enumerate(trainFolder):\n",
        "    #one_hot_vector生成用\n",
        "    tmp = np.zeros(Data_Classes)\n",
        "    if d != \".DS_Store\":\n",
        "        #フォルダ内のファイルのリストを取得\n",
        "        files = os.listdir(train_folder_name + '/'+d)\n",
        "        #print(files)\n",
        "\n",
        "        #one_hot_vectorを作成\n",
        "        tmp[i-dsflag] = 1\n",
        "        #ファイル毎にみる\n",
        "        for f in files:\n",
        "                            \n",
        "            #.DS_Storeをどかす\n",
        "            if f != \".DS_Store\":\n",
        "                #どのファイルを見ているか確認用\n",
        "                #print(\"load:\"+f)\n",
        "\n",
        "                datafile_path = train_folder_name + '/' + d+'/'+f\n",
        "\n",
        "                #csvから読み込み\n",
        "                data = np.loadtxt(datafile_path, delimiter=\",\")     #pbldata用\n",
        "                \n",
        "                \n",
        "                #データ取り出し\n",
        "                csv321 = data[:]\n",
        "             \n",
        "                \n",
        "                flatdata = []\n",
        "                flatdata.append(csv321)\n",
        "          \n",
        "                All_Datas.append(flatdata)\n",
        "                #one_hot_vectorをラベルに追加\n",
        "#                 print(\"load:\"+f+\" ohv: \"+str(tmp))\n",
        "                All_Label.append(tmp)\n",
        "            else:\n",
        "                print(\".ds_storeを除去\")\n",
        "    else:\n",
        "        print(\".ds_storeを抹殺\")\n",
        "        print(i)\n",
        "        dsflag = dsflag + 1\n",
        "\n",
        "\n",
        "dsflag = 0\n",
        "\n",
        "#検証データの読み込み\n",
        "# for i, d in enumerate(testFolder):\n",
        "#     #one_hot_vector生成用\n",
        "#     tmp = np.zeros(Data_Classes)\n",
        "#     if d != \".DS_Store\":\n",
        "#         # フォルダ内のファイルのリストを取得\n",
        "#         files = os.listdir(test_folder_name + '/' + d)\n",
        "#         #print(files)\n",
        "\n",
        "#         #one_hot_vectorを作成\n",
        "#         tmp[i-dsflag] = 1\n",
        "#         #ファイル毎にみる\n",
        "#         for f in files:\n",
        "                            \n",
        "#             #.DS_Storeをどかす\n",
        "#             if f != \".DS_Store\":\n",
        "#                 #どのファイルを見ているか確認用\n",
        "#                 #print(\"load:\"+f)\n",
        "\n",
        "#                 datafile_path = test_folder_name + '/' + d+'/'+f\n",
        "\n",
        "#                 #csvから読み込み\n",
        "#                 data = np.loadtxt(datafile_path, delimiter=\",\")     #pbldata用\n",
        "\n",
        "#                  #3x200のデータに整形\n",
        "#                 x_csv = data[1, :]\n",
        "#                 y_csv = data[2, :]\n",
        "#                 z_csv = data[3, :]\n",
        "                \n",
        "#                 start = random.randint(0, len(x_csv) - CaptureNumber)\n",
        "#                 end = start + CaptureNumber\n",
        "\n",
        "#                 x_data = x_csv[start:end]\n",
        "#                 y_data = y_csv[start:end]\n",
        "#                 z_data = z_csv[start:end]\n",
        "\n",
        "\n",
        "#                 flatdata = []\n",
        "#                 flatdata.append(x_data)\n",
        "#                 flatdata.append(y_data)\n",
        "#                 flatdata.append(z_data)\n",
        "\n",
        "                \n",
        "#                 Ex_TestDatas.append(flatdata)\n",
        "#                 #one_hot_vectorをラベルに追加\n",
        "#                 Ex_Label.append(tmp)\n",
        "#             else:\n",
        "#                 print(\".ds_storeを除去\")\n",
        "#     else:\n",
        "#         print(\"ex.ds_storeを抹殺\")\n",
        "#         dsflag = dsflag + 1\n",
        "  \n",
        "# print(\"Ex_Label\")\n",
        "# print(Ex_Label[0])\n",
        "\n",
        "#訓練データ順列のシャッフル\n",
        "np.random.seed(seed=32)\n",
        "print(len(All_Datas))\n",
        "Rindex = np.random.permutation(list(range(len(All_Datas))))\n",
        "# print(Rindex)\n",
        "for k in Rindex:\n",
        "    All_SDatas.append(All_Datas[k])\n",
        "    All_SLabel.append(All_Label[k])\n",
        "\n",
        "    \n",
        "#訓練データを学習に使うやつとモデルの評価に使うやつの２種に分ける\n",
        "numberV = int(len(All_SDatas)*(Validation)) #訓練データ数\n",
        "testV = len(All_SDatas) - numberV  #テストデータ数\n",
        "train_data = All_SDatas[:numberV] #0.９まで\n",
        "test_data = All_SDatas[numberV+1:]\n",
        "train_label = All_SLabel[:numberV]\n",
        "test_label = All_SLabel[numberV+1:]\n",
        "\n",
        "print(len(train_data))\n",
        "print(len(test_data))\n",
        "\n",
        "print(train_data[0])\n",
        "\n",
        "#----------\n",
        "#numpy行列へ\n",
        "#----------\n",
        "\n",
        "train_data = np.array(train_data)\n",
        "train_label = np.array(train_label)\n",
        "\n",
        "test_data = np.array(test_data)\n",
        "test_label = np.array(test_label)\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "x = tf.placeholder('float', shape=[None,1,CaptureNumber]) #1x200のデータ\n",
        "y_ = tf.placeholder('float', shape=[None, Data_Classes]) #正解ラベル\n",
        "keep_prob = tf.placeholder('float')\n",
        "                                     \n",
        "# -----------------------------------------\n",
        "# 畳み込みニューラルネットワーク\n",
        "#\n",
        "# (convconv -> bn -> pool -> drop) x3，\n",
        "# (fc->drop) x2\n",
        "# -----------------------------------------\n",
        "\n",
        "#第1ブロック\n",
        "W_conv1_1 = weight_variable([1,5,1,filterSize]) #1*5のフィルタ　入力３ｃｈ　出力６４枚\n",
        "b_conv1_1 = bias_variable([filterSize]) #出力６４ch\n",
        "x_image = tf.reshape(x,[-1,1,CaptureNumber,1]) #-１はreshapeに適切な数N　１*２００のデータ　３ｃｈ\n",
        "h_conv1_1 = conv2d(x_image, W_conv1_1)#+b_conv1_1 \n",
        "tan1_1 = Activation(h_conv1_1)\n",
        "bn1_1 = batch_normalization(filterSize,tan1_1)\n",
        "\n",
        "# print(h_conv1_1.shape)\n",
        "\n",
        "W_conv1_2 = weight_variable([1,5,filterSize,filterSize])\n",
        "b_conv1_2 = bias_variable([filterSize])\n",
        "h_conv1_2 = conv2d(bn1_1, W_conv1_2)#+b_conv1_2\n",
        "tan1_2 = Activation(h_conv1_2)\n",
        "bn1_2 = batch_normalization(filterSize,tan1_2)\n",
        "\n",
        "h_pool1 = max_pool_1x2(bn1_2)\n",
        "h_pool1_drop = tf.nn.dropout(h_pool1, keep_prob)\n",
        "\n",
        "# print(h_pool1.shape)\n",
        "\n",
        "\n",
        "# #第2ブロック\n",
        "W_conv2_1 = weight_variable([1,5,filterSize,filterSize*2])\n",
        "b_conv2_1 = bias_variable([filterSize*2])\n",
        "h_conv2_1 = conv2d(h_pool1_drop, W_conv2_1)+b_conv2_1\n",
        "tan2_1 = Activation(h_conv2_1)\n",
        "bn2_1 = batch_normalization(filterSize*2,tan2_1)\n",
        "\n",
        "W_conv2_2 = weight_variable([1,5,filterSize*2,filterSize*2])\n",
        "b_conv2_2 = bias_variable([filterSize*2])\n",
        "h_conv2_2 = conv2d(bn2_1, W_conv2_2)+b_conv2_2\n",
        "tan2_2 = Activation(h_conv2_2)\n",
        "bn2_2 = batch_normalization(filterSize*2,tan2_2)\n",
        "\n",
        "h_pool2 = max_pool_1x2(bn2_2)\n",
        "h_pool2_drop = tf.nn.dropout(h_pool2, keep_prob)\n",
        "\n",
        "# print(h_pool2_drop .shape)\n",
        "\n",
        "#\n",
        "\n",
        "# #第3ブロック\n",
        "W_conv3_1 = weight_variable([1,5,filterSize*2,filterSize*4])\n",
        "b_conv3_1 = bias_variable([filterSize*4])\n",
        "h_conv3_1 = conv2d(h_pool2_drop, W_conv3_1)+b_conv3_1\n",
        "tan3_1 = Activation(h_conv3_1)\n",
        "bn3_1 = batch_normalization(filterSize*4,tan3_1)\n",
        "\n",
        "W_conv3_2 = weight_variable([1,5,filterSize*4,filterSize*4])\n",
        "b_conv3_2 = bias_variable([filterSize*4])\n",
        "h_conv3_2 = conv2d(bn3_1, W_conv3_2)+b_conv3_2\n",
        "tan3_2 = Activation(h_conv3_2)\n",
        "bn3_2 = batch_normalization(filterSize*4,tan3_2)\n",
        "\n",
        "\n",
        "h_pool3 = max_pool_1x2(bn3_2)\n",
        "h_pool3_drop = tf.nn.dropout(h_pool3, keep_prob)\n",
        "print(h_pool3_drop .shape)\n",
        "h_pool3_flat = tf.reshape(h_pool3_drop, [-1,allCSize])\n",
        "\n",
        "\n",
        "# h_pool2_flat = tf.reshape(h_pool2_drop, [-1,allCSize])#\n",
        "\n",
        "#全結合層\n",
        "W_fc1 = weight_variable([allCSize, allCSize])\n",
        "b_fc1 = bias_variable([allCSize])\n",
        "h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1)\n",
        "# h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)#\n",
        "bn4 = batch_normalization(allCSize,h_fc1)\n",
        "\n",
        "# #dropout\n",
        "h_fc1_drop = tf.nn.dropout(bn4, 0.5)\n",
        "\n",
        "# #全結合層\n",
        "W_fc2 = weight_variable([allCSize, allCSize])\n",
        "b_fc2 = bias_variable([allCSize])\n",
        "\n",
        "h_fc2= tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
        "bn5 = batch_normalization(allCSize,h_fc1)\n",
        "\n",
        "# #dropout\n",
        "h_fc2_drop = tf.nn.dropout(bn5, 0.5)\n",
        "\n",
        "#softmaxで出力#\n",
        "W_fc4 = weight_variable([allCSize, Data_Classes])\n",
        "b_fc4 = bias_variable([Data_Classes])\n",
        "y_conv=tf.nn.softmax(tf.matmul(h_fc2_drop, W_fc4) + b_fc4)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#------------------\n",
        "# 評価と損失関数\n",
        "#------------------\n",
        "\n",
        "#L2ノルムを足そうとした残骸\n",
        "# L2_sqr = tf.nn.l2_loss(W_conv1) + tf.nn.l2_loss(W_conv2)+tf.nn.l2_loss(W_conv3) + tf.nn.l2_loss(W_fc1) + tf.nn.l2_loss(W_fc2)\n",
        "# lambda_2 = 0.01\n",
        "\n",
        "#クロスエントロピー誤差関数，1e-7を足して，勾配消失を防止\n",
        "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_*tf.log(y_conv+1e-7),reduction_indices=[1]))\n",
        "#loss = cross_entropy + lambda_2*L2_sqr\n",
        "# cross_entropy = -tf.reduce_sum(y_*tf.log(tf.clip_by_value(y_conv,1e-10,1.0)))\n",
        "\n",
        "#重みの最適化\n",
        "train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\n",
        "#　推定結果のy_convと正解ラベルy_が同じかどうか判定，correct_predictionがbool配列になってる\n",
        "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_,1))\n",
        "# 正答率\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "# run\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "\n",
        "\n",
        "#\n",
        "#学習器の保存処理\n",
        "saver = tf.train.Saver()\n",
        "ckpt = tf.train.get_checkpoint_state('./')\n",
        "cwd = os.getcwd()\n",
        "\n",
        "\n",
        "#処理時間計測\n",
        "startTime = datetime.datetime.today()\n",
        "\n",
        "#ログ\n",
        "learn = open('log.csv','w')\n",
        "result = open('result.txt','w')\n",
        "\n",
        "\n",
        "learn.write(\"class,step\\n\" )\n",
        "learn.write(\"%g,%g\\n\"%(Data_Classes,steps))\n",
        "learn.write(\"step,train_a,test_a\\n\" )\n",
        "\n",
        "#学習ループ\n",
        "#配列に入れた加速度データから，ランダムにデータを選択，さらにそこから1x200に切り出して学習にかける\n",
        "for i in range(steps):\n",
        "\n",
        "    #加速度データの中からランダムにデータを選択\n",
        "    batch_mask = np.random.choice(len(train_data),BATCH_SIZE)\n",
        "    tbatch_mask = np.random.choice(len(test_data),BATCH_SIZE)\n",
        "\n",
        "    #バッチ配列\n",
        "    train_data_batch = []\n",
        "    train_label_batch = train_label[batch_mask]\n",
        "    test_data_batch = []\n",
        "    \n",
        "#     print(len(batch_mask))\n",
        "    #1x200に切り出し_訓練データ\n",
        "    for g in batch_mask:\n",
        "#         rdata = train_data[g]\n",
        "#         train_data_batch.append(rdata)\n",
        "#         print(train_data[g])\n",
        "        b_data = train_data[g]\n",
        "        b_data = b_data[0]\n",
        "\n",
        "#         print(len(b_data))\n",
        "        rdata = []\n",
        "        start = random.randint(0, len(b_data) - CaptureNumber)\n",
        "        end = start + CaptureNumber\n",
        "\n",
        "        x_data = b_data[start:end]\n",
        "      \n",
        "        rdata.append(x_data)\n",
        "        train_data_batch.append(rdata)\n",
        "\n",
        "    # 3x200に切り出し_テストデータ\n",
        "    for g in range(len(test_data)):\n",
        "#         rdata = test_data[g]\n",
        "#         test_data_batch.append(rdata)\n",
        "        b_data = test_data[g]\n",
        "  \n",
        "\n",
        "        b_x = b_data[0]\n",
        "        rdata = []\n",
        "        start = random.randint(0, len(b_x) - CaptureNumber)\n",
        "        end = start + CaptureNumber\n",
        "\n",
        "        x_data = b_x[start:end]\n",
        "        rdata.append(x_data)\n",
        "        test_data_batch.append(rdata)\n",
        "\n",
        "    # 学習の実行　x＝データy＿＝ラベル keep_prob=dropoutでどれだけノード消すか\n",
        "    w_out, _ = sess.run([y_conv, train_step],feed_dict={x: train_data_batch, y_: train_label_batch, keep_prob:drop})\n",
        "\n",
        "    #精度の計算\n",
        "    train_accuracy = accuracy.eval(feed_dict={\n",
        "        x: train_data_batch, y_: train_label_batch, keep_prob:drop})\n",
        "\n",
        "    # 10ステップごとに精度を記録\n",
        "    if i % 10 == 0 :\n",
        "        test_accuracy = accuracy.eval(feed_dict={\n",
        "            x: test_data_batch, y_: test_label, keep_prob: drop})\n",
        "\n",
        "        print(\"step %d, training accuracy %g   test accuracy %g\" % (i, train_accuracy, test_accuracy))\n",
        "        learn.write(\"%d,%g,%g\\n\" % (i, train_accuracy, test_accuracy))\n",
        "\n",
        "    #終了一回前にいろいろ作成\n",
        "    if i == steps-1:\n",
        "\n",
        "        # confusion matrix 作成\n",
        "        y_p = tf.argmax(y_conv, 1)\n",
        "        val_accuracy, y_pred = sess.run([accuracy, y_p], feed_dict={x: test_data_batch, y_: test_label, keep_prob: drop})\n",
        "        y_true = np.argmax(test_label, 1)\n",
        "        con = confusion_matrix(y_true, y_pred)\n",
        "        print(con)\n",
        "\n",
        "        #検証データ分類用\n",
        "        #y_pred_1 = sess.run([y_p],feed_dict={x: Ex_TestDatas, keep_prob: drop})\n",
        "        # y_pred2 = y_pred_1[0]\n",
        "        # print(y_pred2)\n",
        "        # print(len(y_pred2))\n",
        "\n",
        "\n",
        "\n",
        "        print(\"ccn test 正答率 %g\" % accuracy.eval(feed_dict={\n",
        "            x: test_data_batch, y_: test_label, keep_prob: drop\n",
        "        }))\n",
        "\n",
        "#         print(\"ccn realtime test 正答率 %g\" % accuracy.eval(feed_dict={\n",
        "#             x: Ex_TestDatas, y_: Ex_Label, keep_prob: drop\n",
        "#         }))\n",
        "\n",
        "        #処理時間計測，終わり\n",
        "        endTime = datetime.datetime.today()\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"開始時刻 %s\" % (startTime))\n",
        "        print(\"終了時刻 %s\" % (endTime))\n",
        "\n",
        "        learn.write(\"\\n開始時刻 %s\\n\" % (startTime))\n",
        "        learn.write(\"終了時刻 %s\\n\" % (endTime))\n",
        "        learn.write(\"ccn test 正答率 %g\" % accuracy.eval(feed_dict={\n",
        "            x: test_data_batch, y_: test_label, keep_prob: drop\n",
        "        }))\n",
        "\n",
        "        #confusion matrixについて，精度を百分率表記したcon1と何が何回正解したかを表すconの両方つくる\n",
        "        con_sum = np.sum(con, axis=1)\n",
        "        con1 = []\n",
        "        for a in range(len(con)):\n",
        "            c = con[a] / con_sum[a]\n",
        "            con1.append(c)\n",
        "\n",
        "        print(con_sum)\n",
        "        print(con)\n",
        "        result.write(\"Confusion Matrix :\\n\")\n",
        "        # np.savetxt('result.txt',con,fmt=\"%0.2f\")\n",
        "        np.savetxt('confusion-no.csv', con, fmt=\"%0.3f\", delimiter=',')\n",
        "        np.savetxt('confusion.csv', con1, fmt=\"%0.3f\", delimiter=',')\n",
        "\n",
        "\n",
        "        ##学習器の保存処理２\n",
        "saver.save(sess,cwd+\"\\\\0model.ckpt\")\n",
        "\n",
        "\n",
        "\n",
        "learn.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n",
            "9\n",
            "train_dft321\n",
            "test_folder_name\n",
            "802\n",
            "641\n",
            "160\n",
            "[array([0.08691273, 0.08266657, 0.09198814, ..., 0.06081351, 0.07147829,\n",
            "       0.06905096])]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(?, 1, 18, 512)\n",
            "step 0, training accuracy 0.411667   test accuracy 0.2\n",
            "step 10, training accuracy 0.315   test accuracy 0.28125\n",
            "step 20, training accuracy 0.396667   test accuracy 0.3375\n",
            "step 30, training accuracy 0.335   test accuracy 0.325\n",
            "step 40, training accuracy 0.398333   test accuracy 0.4125\n",
            "step 50, training accuracy 0.38   test accuracy 0.39375\n",
            "step 60, training accuracy 0.421667   test accuracy 0.34375\n",
            "step 70, training accuracy 0.421667   test accuracy 0.35\n",
            "step 80, training accuracy 0.383333   test accuracy 0.40625\n",
            "step 90, training accuracy 0.438333   test accuracy 0.3875\n",
            "step 100, training accuracy 0.438333   test accuracy 0.35\n",
            "step 110, training accuracy 0.433333   test accuracy 0.3875\n",
            "step 120, training accuracy 0.443333   test accuracy 0.44375\n",
            "step 130, training accuracy 0.465   test accuracy 0.40625\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}